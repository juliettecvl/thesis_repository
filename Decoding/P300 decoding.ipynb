{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc2eacdc",
   "metadata": {},
   "source": [
    "# P300 decoding\n",
    "Below is the pipeline for the decoding of P300. This notebooks experiments with a few different conditions. First of all, distinctions are made between ICA and non-ICA versions. Secondly, it is researched whether BT-LDA possibly outperforms sLDA in a separate pipeline. The general pipeline has already been provided by Radovan Vodila, with minor adjustments made by Juliette van Lohuizen. This mainly concerns the application of ICA and BT-LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7574afa",
   "metadata": {},
   "source": [
    "## General functions and imports\n",
    "Note that the functions below were all provided by Radovan Vodila. Even though it is still included in the notebook, after experimenting with marking 'bad' epochs, and balancing classes, it was noted that this did not improve performance, and in some cases even decreased it significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dea57a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import warnings\n",
    "from toeplitzlda.classification import ToeplitzLDA\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "derivatives_dir = '/Users/juliette/Desktop/thesis/p300_npz/preprocessed/p300'\n",
    "decoding_results_dir = '/Users/juliette/Desktop/thesis/results/P300'\n",
    "\n",
    "subjects = [\"VPpdia\",\"VPpdib\", \"VPpdic\", \"VPpdid\", \"VPpdie\", \"VPpdif\", \"VPpdig\",\"VPpdih\", \"VPpdii\", \"VPpdij\",\n",
    "            \"VPpdik\", \"VPpdil\", \"VPpdim\", \"VPpdin\", \"VPpdio\", \"VPpdip\", \"VPpdiq\", \"VPpdir\", \"VPpdis\", \"VPpdit\",\n",
    "            \"VPpdiu\", \"VPpdiv\", \"VPpdiw\", \"VPpdix\",\"VPpdiy\", \"VPpdiz\", \"VPpdiza\", \"VPpdizb\", \"VPpdizc\"]\n",
    "\n",
    "# -- Functions to extract the features from P300 and add epochs to the data --\n",
    "def extract_epochs(X, start_idx=60, end_idx=2460, step_size=30, \n",
    "                   start_sample_offset=-24, end_sample_offset=84, \n",
    "                   amplitude_threshold=40e-6):\n",
    "    \"\"\"\n",
    "    Function to extract epochs from time-series data for ERP features, \n",
    "    baseline-correct each epoch, and identify bad epochs based on amplitude threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Input data array of shape (n_trials, n_channels, n_samples)\n",
    "    - start_idx: The starting sample index for the first epoch (default=120)\n",
    "    - end_idx: The last sample index where the final epoch starts (default=2520)\n",
    "    - step_size: Step size in samples, corresponding to the sliding window (default=30)\n",
    "    - start_sample_offset: The offset for the start of the time window (default=-24, corresponds to -200 ms)\n",
    "    - end_sample_offset: The offset for the end of the time window (default=84, corresponds to 700 ms)\n",
    "    - amplitude_threshold: Threshold for identifying bad epochs based on amplitude range (default=100)\n",
    "    \n",
    "    Returns:\n",
    "    - output_matrix: A 4D array of extracted and baseline-corrected epochs of shape \n",
    "                     (n_trials, n_epochs, n_channels, window_size)\n",
    "    - bad_epochs_idx: List of indices of bad epochs for each trial and channel \n",
    "                      where amplitude range exceeds the threshold.\n",
    "    \"\"\"\n",
    "    # Check input dimensions\n",
    "    if X.ndim != 3:\n",
    "        raise ValueError(f\"Input X must have 3 dimensions (n_trials, n_channels, n_samples), but got {X.ndim} dimensions.\")\n",
    "    \n",
    "    n_trials, n_channels, n_samples = X.shape\n",
    "    window_size = end_sample_offset + np.abs(start_sample_offset)  # 108 samples\n",
    "    epoch_timestamps = np.arange(start_idx, end_idx, step_size)    # (80,)\n",
    "    n_epochs = len(epoch_timestamps)\n",
    "    \n",
    "    # Initialize the output matrix for the epochs and a list for bad epoch indices\n",
    "    output_matrix = np.zeros((n_trials, n_epochs, n_channels, window_size))\n",
    "    bad_epochs_idx = []  # To store (trial, epoch, channel) indices of bad epochs\n",
    "    \n",
    "    # Loop over trials, channels, and epochs to extract and baseline-correct the windows\n",
    "    for i_trial in range(n_trials):\n",
    "        for i_channel in range(n_channels):\n",
    "            data = X[i_trial, i_channel, :]\n",
    "\n",
    "            for i_epoch, t in enumerate(epoch_timestamps):\n",
    "                epoch_start_idx = t + start_sample_offset  # Start at t - 24 samples (-200 ms)\n",
    "                epoch_end_idx = t + end_sample_offset      # End at t + 84 samples (700 ms)\n",
    "                \n",
    "                # Ensure the window stays within bounds\n",
    "                if epoch_start_idx >= 0 and epoch_end_idx <= n_samples:\n",
    "                    epoch_data = data[epoch_start_idx:epoch_end_idx]\n",
    "                    \n",
    "                    # Baseline correction\n",
    "                    baseline_mean = np.mean(epoch_data[:25])\n",
    "                    epoch_data = epoch_data - baseline_mean\n",
    "                    \n",
    "                    # Store the epoch in the output matrix\n",
    "                    output_matrix[i_trial, i_epoch, i_channel, :] = epoch_data\n",
    "                    \n",
    "                    # Check amplitude range after baseline subtraction\n",
    "                    min_amp, max_amp = np.min(epoch_data), np.max(epoch_data)\n",
    "                    amplitude_range = max_amp - min_amp\n",
    "                    \n",
    "                    # Log bad epochs if amplitude range exceeds threshold\n",
    "                    if amplitude_range > amplitude_threshold:\n",
    "                        bad_epochs_idx.append((i_trial, i_epoch, i_channel))\n",
    "    \n",
    "    # Return the 4D output matrix and the indices of bad epochs\n",
    "    return output_matrix, bad_epochs_idx\n",
    "\n",
    "def extract_features_from_X(X_matrix, ToI = None):\n",
    "    \"\"\"\n",
    "    Extracts the maximum amplitudes from specified time ranges for each trial, epoch, and channel in the input data.\n",
    "\n",
    "    Parameters:\n",
    "    - X_matrix: A 4D numpy array of shape (n_trials, n_epochs, n_channels, n_samples) representing the input data.\n",
    "    - ToI: A list of tuples, where each tuple contains the start and end indices of a time range of interest.\n",
    "\n",
    "    Returns:\n",
    "    - feature_matrix: A 4D numpy array of shape (n_trials, n_epochs, n_channels, len(ToI)) containing the maximum\n",
    "                      values from the specified time ranges for each trial, epoch, and channel.\n",
    "    \"\"\"\n",
    "    # Extract the shape of the input matrix\n",
    "    n_trials, n_epochs, n_channels, n_samples = X_matrix.shape \n",
    "    \n",
    "    # Initialize the feature matrix to store maximum values for each time range\n",
    "    feature_matrix = np.zeros((n_trials, n_epochs, n_channels, len(ToI)))\n",
    "\n",
    "    # Loop over the time ranges (ToI) and extract the max value for each range\n",
    "    for i_range, (start, end) in enumerate(ToI):\n",
    "        # For each time range, find the maximum values along the last axis (time samples) in the specified range\n",
    "        feature_matrix[ :, :, :, i_range] = np.mean((X_matrix[ :, :, :, start:end]), axis=-1)\n",
    "\n",
    "    # Return the feature matrix\n",
    "    return feature_matrix\n",
    "\n",
    "# -- Functions fo balancing clases and marking/filtering bad epochs, these are not used anymore --\n",
    "def balance_classes(X, y, ratio_0_to_1=1.0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Sub-select X and y based on a specified ratio of 0s to 1s, keeping the original order.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "    y (numpy.ndarray): Label vector of shape (n_samples,).\n",
    "    ratio_0_to_1 (float): The desired ratio of 0s to 1s in the balanced dataset.\n",
    "\n",
    "    Returns:\n",
    "    X_balanced, y_balanced: Sub-selected feature matrix and label vector.\n",
    "    \"\"\"\n",
    "    # Step 1: Identify indices of 0s and 1s\n",
    "    indices_0 = np.where(y == 0)[0]\n",
    "    indices_1 = np.where(y == 1)[0]\n",
    "    \n",
    "    # Step 2: Calculate the number of samples to select for each class\n",
    "    num_1s = len(indices_1)\n",
    "    num_0s = min(len(indices_0), int(num_1s * ratio_0_to_1))\n",
    "    \n",
    "    # Step 3: Randomly sample the desired number of 0s and 1s\n",
    "    selected_indices_0 = np.random.choice(indices_0, num_0s, replace=False)\n",
    "    selected_indices_1 = np.random.choice(indices_1, num_1s, replace=False)\n",
    "    \n",
    "    # Step 4: Combine selected indices and sort to preserve original order\n",
    "    balanced_indices = np.sort(np.concatenate([selected_indices_0, selected_indices_1]))\n",
    "    \n",
    "    # Step 5: Sub-select X and y based on the balanced indices\n",
    "    X_balanced = X[balanced_indices]\n",
    "    y_balanced = y[balanced_indices]\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "def filter_valid_epochs(X, y, z=None, return_mask=False):\n",
    "    \"\"\"\n",
    "    Filters out epochs where either the features in X or the labels in y contain NaN values.\n",
    "    Optionally, if a z array is provided, it is filtered similarly.\n",
    "    \n",
    "    Parameters:\n",
    "        X (np.ndarray): A 2D numpy array with shape (n_epochs, n_features).\n",
    "        y (np.ndarray): A 1D numpy array with shape (n_epochs,).\n",
    "        z (np.ndarray, optional): An array that will be filtered using the same mask.\n",
    "        return_mask (bool, optional): If True, the boolean mask used for filtering is returned.\n",
    "    \n",
    "    Returns:\n",
    "        filtered_X (np.ndarray): X with only rows that have no NaN values.\n",
    "        filtered_y (np.ndarray): y with only entries corresponding to valid epochs.\n",
    "        filtered_z (np.ndarray or None): Filtered z array (if provided) or None.\n",
    "        mask (np.ndarray, optional): The boolean mask of valid epochs; only returned if return_mask=True.\n",
    "    \"\"\"\n",
    "    # Create a mask for valid labels and features\n",
    "    valid_label_mask = ~np.isnan(y)\n",
    "    valid_feature_mask = ~np.isnan(X).any(axis=1)\n",
    "    combined_mask = valid_label_mask & valid_feature_mask\n",
    "\n",
    "    # Apply the mask to X and y\n",
    "    filtered_X = X[combined_mask]\n",
    "    filtered_y = y[combined_mask]\n",
    "    \n",
    "    if z is not None:\n",
    "        filtered_z = z[combined_mask]\n",
    "    else:\n",
    "        filtered_z = None\n",
    "\n",
    "    if return_mask:\n",
    "        return filtered_X, filtered_y, filtered_z, combined_mask\n",
    "    else:\n",
    "        return filtered_X, filtered_y, filtered_z\n",
    "    \n",
    "def mark_bad_epochs(X, z, bad_idx):\n",
    "    \"\"\"\n",
    "    Marks bad epochs in both EEG data (X) and labels (z) by setting them to NaN (or another sentinel, ie -1).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        4D array of shape (n_trials, n_epochs, n_channels, n_timepoints).\n",
    "    z : ndarray\n",
    "        3D array of shape (n_trials, n_epochs, label_dim).\n",
    "    bad_idx : list of tuples\n",
    "        List of (trial, epoch, channel) indices indicating bad epochs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_marked : ndarray\n",
    "        Same shape as X, with bad epochs set to NaN (or a chosen sentinel).\n",
    "    z_marked : ndarray\n",
    "        Same shape as z, with bad epochs set to NaN (or a chosen sentinel).\n",
    "    \"\"\"\n",
    "    # Convert list of (trial, epoch, channel) to a set of (trial, epoch) pairs\n",
    "    bad_trial_epoch_pairs = set((trial, epoch) for trial, epoch, _ in bad_idx)\n",
    "\n",
    "    # Make copies so we don't overwrite the original arrays\n",
    "    X_marked = np.copy(X)\n",
    "    z_marked = np.copy(z).astype(np.float64)\n",
    "\n",
    "    # Mark each bad epoch in both X and z\n",
    "    for trial_idx, epoch_idx in bad_trial_epoch_pairs:\n",
    "        X_marked[trial_idx, epoch_idx, :, :] = np.nan \n",
    "        z_marked[trial_idx, epoch_idx, :]    = np.nan \n",
    "\n",
    "    return X_marked, z_marked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53c12a",
   "metadata": {},
   "source": [
    "# Extracting P300 features and adding epochs to the data\n",
    "Below is a cell that segments the continuous data into epochs and computes the features within every predefined window segment. This is done over all epochs, channels and trials.\n",
    "\n",
    "## Without ICA\n",
    "First the version without ICA is given. Due to time constraints, I decided not to further optimize this part, as this means I would have to rerun everything, including the entire feature extraction process. However, for future work, this could be streamlined simply by including a parameter that dictates whether ICA is applied or not. The only differences would then be in the paths for loading in the data and saving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rejection_threshold = 1\n",
    "task = \"covert\"  \n",
    "\n",
    "derivatives_dir = '/Users/juliette/Desktop/thesis/preprocessing'\n",
    "\n",
    "for subject in subjects:\n",
    "    print(\"subject:\", subject)\n",
    "    \n",
    "    # Load the NPZ file\n",
    "    file_dir = os.path.join(derivatives_dir, \"p300_preprocessing\")\n",
    "    file_path = os.path.join(file_dir, f\"sub-{subject}_task-{task}_p300.npz\") \n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    npz_data = np.load(file_path)\n",
    "\n",
    "    # Extract data from the npz object\n",
    "    X = npz_data['X']  # EEG data: trials x channels x samples\n",
    "    y = npz_data['y']  # Labels indicating cued side: trials\n",
    "    z = npz_data['z']  # Left and right targets: trials x epochs x sides\n",
    "    fs = npz_data['fs']  # Sampling frequency\n",
    "\n",
    "\n",
    "    X_matrix, bad_idx = extract_epochs(X, amplitude_threshold=rejection_threshold)\n",
    "    '''\n",
    "    Tag bad epochs by setting the values of above-threshold values to NaN\n",
    "    Either use marking like this or alternatively remove it at this stage. Key to retain trial structure!!\n",
    "    Inhomogenity in epoch-dimension size lead to error when constructing np-based matrix, cannot flatten trials & epoch dimenstions as this discards trial strcture \n",
    "    Alternative solution: Build list of arrays and save individually. Decoding needs to be adapted for this data structure.\n",
    "    '''\n",
    "\n",
    "    X_matrix, z = mark_bad_epochs(X_matrix, z, bad_idx)\n",
    "    \n",
    "    # Define periods for feature extraction\n",
    "    ToI = [(30, 38), (38, 48), (48, 57), (57, 69), (69, 87), (87, 108)]\n",
    "\n",
    "    feature_matrix = extract_features_from_X(X_matrix, ToI)\n",
    "\n",
    "    # Save in one NPZ object\n",
    "    save_dir = os.path.join(derivatives_dir, \"features\", \"p300\", f\"sub-{subject}\")\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    np.savez(os.path.join(save_dir, f\"sub-{subject}_task-{task}_p300_features.npz\"), X=feature_matrix, y=y, z=z, fs=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f08211",
   "metadata": {},
   "source": [
    "## With ICA\n",
    "Below is the version with ICA applied. Note that it is the exact same except for the paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece16ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rejection_threshold = 1\n",
    "task = \"covert\"  \n",
    "subjects = [\"VPpdia\",\"VPpdib\", \"VPpdic\", \"VPpdid\", \"VPpdie\", \"VPpdif\", \"VPpdig\",\"VPpdih\", \"VPpdii\", \"VPpdij\",\n",
    "            \"VPpdik\", \"VPpdil\", \"VPpdim\", \"VPpdin\", \"VPpdio\", \"VPpdip\", \"VPpdiq\", \"VPpdir\", \"VPpdis\", \"VPpdit\",\n",
    "            \"VPpdiu\", \"VPpdiv\", \"VPpdiw\", \"VPpdix\",\"VPpdiy\", \"VPpdiz\", \"VPpdiza\", \"VPpdizb\", \"VPpdizc\"]\n",
    "\n",
    "derivatives_dir = '/Users/juliette/Desktop/thesis/preprocessing/p300_preprocessing/p300_ICA'\n",
    "save = '/Users/juliette/Desktop/thesis/preprocessing/features/with_ICA'\n",
    "\n",
    "for subject in subjects:\n",
    "    print(\"subject:\", subject)\n",
    "    \n",
    "    # Load the NPZ file\n",
    "    file_path = os.path.join(derivatives_dir, f\"sub-{subject}_task-{task}_p300_ICA.npz\") \n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    npz_data = np.load(file_path)\n",
    "\n",
    "    # Extract data from the npz object\n",
    "    X = npz_data['X']  # EEG data: trials x channels x samples\n",
    "    y = npz_data['y']  # Labels indicating cued side: trials\n",
    "    z = npz_data['z']  # Left and right targets: trials x epochs x sides\n",
    "    fs = npz_data['fs']  # Sampling frequency\n",
    "\n",
    "    X_matrix, bad_idx = extract_epochs(X, amplitude_threshold=rejection_threshold)\n",
    "    '''\n",
    "    Tag bad epochs by setting the values of above-threshold values to NaN\n",
    "    Either use marking like this or alternatively remove it at this stage. Key to retain trial structure!!\n",
    "    Inhomogenity in epoch-dimension size lead to error when constructing np-based matrix, cannot flatten trials & epoch dimenstions as this discards trial strcture \n",
    "    Alternative solution: Build list of arrays and save individually. Decoding needs to be adapted for this data structure.\n",
    "    '''\n",
    "    X_matrix, z = mark_bad_epochs(X_matrix, z, bad_idx)\n",
    "    \n",
    "    # Define periods for feature extraction\n",
    "    ToI = [(30, 38), (38, 48), (48, 57), (57, 69), (69, 87), (87, 108)]\n",
    "\n",
    "    feature_matrix = extract_features_from_X(X_matrix, ToI)\n",
    "\n",
    "    # Save in one NPZ object\n",
    "    save_dir = os.path.join(save, f\"sub-{subject}\")\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    np.savez(os.path.join(save_dir, f\"sub-{subject}_task-{task}_p300_features_ICA.npz\"), X=feature_matrix, y=y, z=z, fs=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a8afd2",
   "metadata": {},
   "source": [
    "# Decoding pipelines\n",
    "Now the decoding pipelines for P300 are presented. The application of ICA is controlled using a parameter $ica$. Furthermore, experiments were done with two versions of LDA: sLDA and BT-LDA. First, the pipeline of sLDA is presented and then that of BT-LDA. Note that they are very similar, and can easily be streamlined in the future.\n",
    "\n",
    "## P300 decoding with sLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b35b5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: VPpdia\n",
      "Accuracy: 0.9125\n",
      "Subject: VPpdib\n",
      "Accuracy: 0.975\n",
      "Subject: VPpdic\n",
      "Accuracy: 0.95\n",
      "Subject: VPpdid\n",
      "Accuracy: 0.975\n",
      "Subject: VPpdie\n",
      "Accuracy: 0.95\n",
      "Subject: VPpdif\n",
      "Accuracy: 0.9875\n",
      "Subject: VPpdig\n",
      "Accuracy: 0.9625\n",
      "Subject: VPpdih\n",
      "Accuracy: 1.0\n",
      "Subject: VPpdii\n",
      "Accuracy: 0.925\n",
      "Subject: VPpdij\n",
      "Accuracy: 1.0\n",
      "Subject: VPpdik\n",
      "Accuracy: 1.0\n",
      "Subject: VPpdil\n",
      "Accuracy: 0.975\n",
      "Subject: VPpdim\n",
      "Accuracy: 1.0\n",
      "Subject: VPpdin\n",
      "Accuracy: 0.9625\n",
      "Subject: VPpdio\n",
      "Accuracy: 1.0\n",
      "Subject: VPpdip\n",
      "Accuracy: 1.0\n",
      "Subject: VPpdiq\n",
      "Accuracy: 1.0\n",
      "Subject: VPpdir\n",
      "Accuracy: 0.9875\n",
      "Subject: VPpdis\n",
      "Accuracy: 0.9375\n",
      "Subject: VPpdit\n",
      "Accuracy: 0.9375\n",
      "Subject: VPpdiu\n",
      "Accuracy: 0.9875\n",
      "Subject: VPpdiv\n",
      "Accuracy: 1.0\n",
      "Subject: VPpdiw\n",
      "Accuracy: 1.0\n",
      "Subject: VPpdix\n",
      "Accuracy: 0.9625\n",
      "Subject: VPpdiy\n",
      "Accuracy: 0.9625\n",
      "Subject: VPpdiz\n",
      "Accuracy: 1.0\n",
      "Subject: VPpdiza\n",
      "Accuracy: 0.9875\n",
      "Subject: VPpdizb\n",
      "Accuracy: 0.95\n",
      "Subject: VPpdizc\n",
      "Accuracy: 0.9625\n",
      "Overall LDA PR-AUC: 0.56 ± 0.02\n",
      "Overall trial level inference accuracy: 0.97 ± 0.00\n",
      "No trials discarded.\n"
     ]
    }
   ],
   "source": [
    "# -- Parameters -- \n",
    "discard_threshold = 20\n",
    "do_balance_classes = True\n",
    "ratio_0_to_1 = 5\n",
    "ica = False\n",
    "\n",
    "discarded_trials = []\n",
    "results_decoding = {\"subjects\": []}\n",
    "task = 'covert'\n",
    "\n",
    "for subject in subjects:\n",
    "    print(\"Subject:\", subject)\n",
    "    \n",
    "    if ica is True:\n",
    "        file_path = os.path.join('/Users/juliette/Desktop/thesis/preprocessing/features/with_ICA', f\"sub-{subject}\", \n",
    "                    f\"sub-{subject}_task-{task}_p300_features_ICA.npz\")\n",
    "        \n",
    "    else:\n",
    "        file_path = os.path.join('/Users/juliette/Desktop/thesis/preprocessing/features/p300', f\"sub-{subject}\", \n",
    "                    f\"sub-{subject}_task-{task}_p300_features.npz\")\n",
    "        \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Load the data from the NPZ file\n",
    "    npz_data = np.load(file_path)\n",
    "    # Extract data\n",
    "    X = npz_data['X']  # EEG data: trials x epochs x channels x features\n",
    "    y = npz_data['y']  # Labels indicating cued side: trials\n",
    "    z = npz_data['z']  # Left and right targets: trials x epochs x sides\n",
    "\n",
    "    \n",
    "    # Cross-validation setup\n",
    "    n_folds = 4\n",
    "    n_trials = int(X.shape[0] / n_folds)\n",
    "    folds = np.repeat(np.arange(n_folds), n_trials)\n",
    "\n",
    "    fold_pr_auc = []\n",
    "    fold_correct_trials = []\n",
    "\n",
    "    # Cross-validation loop alike in pyntbci\n",
    "    discarded_trial_counter = 0\n",
    "    for i_fold in range(n_folds):\n",
    "    # Split into train and test sets\n",
    "        X_trn_trials, y_trn_trials, z_trn_trials = X[folds != i_fold], y[folds != i_fold], z[folds != i_fold]\n",
    "        X_tst_trials, y_tst_trials, z_tst_trials = X[folds == i_fold], y[folds == i_fold], z[folds == i_fold]\n",
    "        \n",
    "        ### TRAINING\n",
    "        # Flatten training trials into epochs: shape becomes [n_trials * epochs, channel * features]\n",
    "        X_trn_epochs = X_trn_trials.reshape(-1, X_trn_trials.shape[2] * X_trn_trials.shape[3])\n",
    "        \n",
    "        # Extract labels for training epochs using z and y \n",
    "        trial_indices_trn = np.arange(len(y_trn_trials))  # indices for trials\n",
    "        y_trn_epochs = z_trn_trials[trial_indices_trn, :, y_trn_trials].reshape(-1)\n",
    "        \n",
    "        # Filter training epochs\n",
    "        X_trn_epochs, y_trn_epochs, _ = filter_valid_epochs(X_trn_epochs, y_trn_epochs)\n",
    "        \n",
    "        # Balance training classes\n",
    "        if do_balance_classes:\n",
    "            X_trn_epochs, y_trn_epochs = balance_classes(X_trn_epochs, y_trn_epochs, ratio_0_to_1=ratio_0_to_1)\n",
    "        \n",
    "        # Sanity check for training data\n",
    "        if np.isnan(X_trn_epochs).any() or np.isnan(y_trn_epochs).any():\n",
    "            raise ValueError(\"NaNs found in training epochs after filtering!\")\n",
    "        \n",
    "        ## Fit LDA\n",
    "        lda = LDA(solver=\"lsqr\", covariance_estimator=LedoitWolf())\n",
    "        # Dimensionality bust be 2\n",
    "        lda.fit(X_trn_epochs, y_trn_epochs)\n",
    "\n",
    "        ### TESTING\n",
    "        # Flatten testing trials into epochs\n",
    "        X_tst_epochs = X_tst_trials.reshape(-1, X_tst_trials.shape[2] * X_tst_trials.shape[3])\n",
    "        \n",
    "        # Extract labels for testing epochs (again, using z and y for indexing)\n",
    "        trial_indices_tst = np.arange(len(y_tst_trials))\n",
    "        y_tst_epochs = z_tst_trials[trial_indices_tst, :, y_tst_trials].reshape(-1)\n",
    "        \n",
    "        # Reshape z_tst_trials into epochs.\n",
    "        z_tst_epochs = z_tst_trials.reshape(len(y_tst_trials) * z_tst_trials.shape[1], 2)\n",
    "        \n",
    "        # Filter testing epochs and also retrieve the original mask so we can count epochs per trial\n",
    "        X_tst_epochs, y_tst_epochs, z_tst_epochs, combined_mask_tst = filter_valid_epochs(\n",
    "            X_tst_epochs, y_tst_epochs, z=z_tst_epochs, return_mask=True\n",
    "        )\n",
    "        \n",
    "        # Calculate the number of preserved epochs per trial for testing.\n",
    "        # Here, combined_mask_tst still has the original shape before filtering.\n",
    "        # Reshape it to [n_trials, epochs_per_trial] and sum True values per trial.\n",
    "        epoch_counts = combined_mask_tst.reshape(len(y_tst_trials), -1)\n",
    "        num_epochs = np.sum(epoch_counts, axis=1)\n",
    "        \n",
    "        # Rebuild trial structure for testing data based on num_epochs\n",
    "        nested_X_tst_trials = []\n",
    "        nested_z_tst_epochs = []\n",
    "        start_idx = 0\n",
    "        for trial_idx, n_ep in enumerate(num_epochs):\n",
    "            end_idx = start_idx + n_ep\n",
    "            nested_X_tst_trials.append(X_tst_epochs[start_idx:end_idx])\n",
    "            nested_z_tst_epochs.append(z_tst_epochs[start_idx:end_idx])\n",
    "            start_idx = end_idx\n",
    "        \n",
    "\n",
    "        # Evaluate model on test data\n",
    "        correct_trials = 0\n",
    "        \n",
    "        for t_idx in range(len(y_tst_trials)):\n",
    "\n",
    "            num_preserved_epochs = num_epochs[t_idx]\n",
    "            if num_preserved_epochs < discard_threshold:\n",
    "                discarded_trial_counter +=1\n",
    "                continue\n",
    "            \n",
    "            # Log cued side informed by y_tst\n",
    "            cued_side = y_tst_trials[t_idx]\n",
    "            # create event vectors & ground truth\n",
    "            left_targets = nested_z_tst_epochs[t_idx][:, 0]\n",
    "            right_targets = nested_z_tst_epochs[t_idx][:, 1]\n",
    "            cued_targets = nested_z_tst_epochs[t_idx] [:, cued_side]\n",
    "            \n",
    "   \n",
    "            # Compute LDA scores for epochs\n",
    "            epoch_scores = lda.decision_function(nested_X_tst_trials[t_idx])\n",
    "            \n",
    "            # Log performance per fold\n",
    "            precision, recall, _ = precision_recall_curve(cued_targets, epoch_scores)\n",
    "            pr_auc_score = auc(recall, precision)\n",
    "            fold_pr_auc.append(pr_auc_score)\n",
    "\n",
    "            # Correlation-based decision\n",
    "            corr_left, _ = pearsonr(epoch_scores, left_targets)\n",
    "            corr_right, _ = pearsonr(epoch_scores, right_targets)\n",
    "\n",
    "            # Trial-level decision rule based on correlation\n",
    "            decision = 0 if corr_left > corr_right else 1\n",
    "            if decision == cued_side:\n",
    "                correct_trials += 1\n",
    "        \n",
    "        fold_correct_trials.append(correct_trials)\n",
    "    discarded_trials.append([subject, discarded_trial_counter/ n_folds])\n",
    "    # Compute perf metrics\n",
    "    pr_auc_mean = np.mean(fold_pr_auc)\n",
    "    pr_auc_se = np.std(fold_pr_auc, ddof=1) / np.sqrt(len(fold_pr_auc))\n",
    "    correct_trials_mean = np.mean(fold_correct_trials) / (y.size / n_folds)\n",
    "    correct_trials_se = np.std(fold_correct_trials, ddof=1) / np.sqrt(len(fold_correct_trials)) / (y.size / n_folds)\n",
    "    print(\"Accuracy:\", correct_trials_mean)\n",
    "        \n",
    "     # --- Prepare Data for saving ---\n",
    "    # Store results\n",
    "    results_decoding[\"subjects\"].append({\n",
    "        \"subject_id\": subject,\n",
    "        \"pr_auc_mean\": pr_auc_mean,\n",
    "        \"pr_auc_se\": pr_auc_se,\n",
    "        \"correct_trials_mean\": correct_trials_mean,\n",
    "        \"correct_trials_se\": correct_trials_se\n",
    "    })\n",
    "    # --- Add Aggregate (Overall) Results Before Saving ---\n",
    "pr_auc_means = [subj['pr_auc_mean'] for subj in results_decoding[\"subjects\"]]\n",
    "correct_trials_means = [subj['correct_trials_mean'] for subj in results_decoding[\"subjects\"]]\n",
    "\n",
    "overall_result = {\n",
    "    \"subject_id\": \"Overall\",\n",
    "    \"pr_auc_mean\": np.mean(pr_auc_means),\n",
    "    \"pr_auc_se\": np.std(pr_auc_means, ddof=1) / np.sqrt(len(pr_auc_means)),\n",
    "    \"correct_trials_mean\": np.mean(correct_trials_means),\n",
    "    \"correct_trials_se\": np.std(correct_trials_means, ddof=1) / np.sqrt(len(correct_trials_means))\n",
    "}\n",
    "results_decoding[\"subjects\"].append(overall_result)\n",
    "\n",
    "# Convert results to a structured NumPy array for saving\n",
    "dtype = [\n",
    "    ('subject_id', 'U10'),   # Unicode string (max length 10)\n",
    "    ('pr_auc_mean', 'f8'),\n",
    "    ('pr_auc_se', 'f8'),\n",
    "    ('correct_trials_mean', 'f8'),\n",
    "    ('correct_trials_se', 'f8')\n",
    "]\n",
    "structured_array = np.array(\n",
    "    [\n",
    "        (\n",
    "            subj['subject_id'],\n",
    "            subj['pr_auc_mean'],\n",
    "            subj['pr_auc_se'],\n",
    "            subj['correct_trials_mean'],\n",
    "            subj['correct_trials_se']\n",
    "        )\n",
    "        for subj in results_decoding['subjects']\n",
    "    ],\n",
    "    dtype=dtype\n",
    ")\n",
    "\n",
    "# Save the structured array\n",
    "if not os.path.exists(decoding_results_dir):\n",
    "    os.makedirs(decoding_results_dir)\n",
    "    \n",
    "if ica is True:\n",
    "    decoding_results_dir = '/Users/juliette/Desktop/thesis/results/P300/p300_ICA'\n",
    "    np.save(join(decoding_results_dir, f\"{task}_lda_p300_ICA_results.npy\"), structured_array)\n",
    "    \n",
    "else:\n",
    "    np.save(join(decoding_results_dir, f\"{task}_lda_p300_results.npy\"), structured_array)\n",
    "\n",
    "# --- Print Overall Metrics ---\n",
    "overall_pr_auc_mean = overall_result['pr_auc_mean']\n",
    "overall_pr_auc_se = overall_result['pr_auc_se']\n",
    "overall_accuracy_mean = overall_result['correct_trials_mean']\n",
    "overall_accuracy_se = overall_result['correct_trials_se']\n",
    "\n",
    "print(f\"Overall LDA PR-AUC: {overall_pr_auc_mean:.2f} ± {overall_pr_auc_se:.2f}\")\n",
    "print(f\"Overall trial level inference accuracy: {overall_accuracy_mean:.2f} ± {overall_accuracy_se:.2f}\")\n",
    "\n",
    "# --- Log Discarded Trials ---\n",
    "\n",
    "discarded_trials_log = [(subj, val) for subj, val in discarded_trials if val > (1 / 4)] # account for fold-average\n",
    "if discarded_trials_log:\n",
    "    print(f\"Trials discarded (< {discard_threshold} epochs available for integration) for the following subjects (fold-average):\")\n",
    "    for subj, val in discarded_trials_log:\n",
    "        print(f\"Subject: {subj}, Discarded Trials: {val}\")\n",
    "else:\n",
    "    print(\"No trials discarded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca51f228",
   "metadata": {},
   "source": [
    "## P300 decoding with BT-LDA\n",
    "When using BT-LDA, ICA is always applied. The rest of the pipeline remains similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "113a9a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject: VPpdia\n",
      "Accuracy: 0.9125\n",
      "subject: VPpdib\n",
      "Accuracy: 0.9875\n",
      "subject: VPpdic\n",
      "Accuracy: 0.975\n",
      "subject: VPpdid\n",
      "Accuracy: 0.975\n",
      "subject: VPpdie\n",
      "Accuracy: 0.925\n",
      "subject: VPpdif\n",
      "Accuracy: 0.9875\n",
      "subject: VPpdig\n",
      "Accuracy: 0.95\n",
      "subject: VPpdih\n",
      "Accuracy: 1.0\n",
      "subject: VPpdii\n",
      "Accuracy: 0.95\n",
      "subject: VPpdij\n",
      "Accuracy: 1.0\n",
      "subject: VPpdik\n",
      "Accuracy: 1.0\n",
      "subject: VPpdil\n",
      "Accuracy: 0.9375\n",
      "subject: VPpdim\n",
      "Accuracy: 1.0\n",
      "subject: VPpdin\n",
      "Accuracy: 0.9625\n",
      "subject: VPpdio\n",
      "Accuracy: 0.9875\n",
      "subject: VPpdip\n",
      "Accuracy: 1.0\n",
      "subject: VPpdiq\n",
      "Accuracy: 1.0\n",
      "subject: VPpdir\n",
      "Accuracy: 1.0\n",
      "subject: VPpdis\n",
      "Accuracy: 0.9625\n",
      "subject: VPpdit\n",
      "Accuracy: 0.9375\n",
      "subject: VPpdiu\n",
      "Accuracy: 0.9875\n",
      "subject: VPpdiv\n",
      "Accuracy: 1.0\n",
      "subject: VPpdiw\n",
      "Accuracy: 0.9875\n",
      "subject: VPpdix\n",
      "Accuracy: 0.9625\n",
      "subject: VPpdiy\n",
      "Accuracy: 0.9875\n",
      "subject: VPpdiz\n",
      "Accuracy: 0.975\n",
      "subject: VPpdiza\n",
      "Accuracy: 1.0\n",
      "subject: VPpdizb\n",
      "Accuracy: 0.9625\n",
      "subject: VPpdizc\n",
      "Accuracy: 0.9875\n",
      "Overall LDA PR-AUC: 0.53 ± 0.02\n",
      "Overall trial level inference accuracy: 0.98 ± 0.00\n",
      "No trials discarded.\n"
     ]
    }
   ],
   "source": [
    "# -- Parameters -- \n",
    "discard_threshold = 20\n",
    "do_balance_classes = True\n",
    "ratio_0_to_1 = 5\n",
    "\n",
    "decoding_results_dir = '/Users/juliette/Desktop/thesis/results/P300/p300_ICA'\n",
    "\n",
    "discarded_trials = []\n",
    "results_decoding = {\"subjects\": []}\n",
    "task = 'covert'\n",
    "for subject in subjects:\n",
    "    print(\"subject:\", subject)\n",
    "    file_path = os.path.join('/Users/juliette/Desktop/thesis/preprocessing/features/with_ICA', f\"sub-{subject}\", \n",
    "                            f\"sub-{subject}_task-{task}_p300_features_ICA.npz\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "     # Load the data from the NPZ file\n",
    "    npz_data = np.load(file_path)\n",
    "    # Extract data\n",
    "    X = npz_data['X']  # EEG data: trials x epochs x channels x features\n",
    "    y = npz_data['y']  # Labels indicating cued side: trials\n",
    "    z = npz_data['z']  # Left and right targets: trials x epochs x sides\n",
    "    \n",
    "    # Cross-validation setup\n",
    "    n_folds = 4\n",
    "    n_trials = int(X.shape[0] / n_folds)\n",
    "    folds = np.repeat(np.arange(n_folds), n_trials)\n",
    "    n_channels = X.shape[2]\n",
    "\n",
    "    fold_pr_auc = []\n",
    "    fold_correct_trials = []\n",
    "\n",
    "    # Cross-validation loop alike in pyntbci\n",
    "    discarded_trial_counter = 0\n",
    "    for i_fold in range(n_folds):\n",
    "    # Split into train and test sets\n",
    "        X_trn_trials, y_trn_trials, z_trn_trials = X[folds != i_fold], y[folds != i_fold], z[folds != i_fold]\n",
    "        X_tst_trials, y_tst_trials, z_tst_trials = X[folds == i_fold], y[folds == i_fold], z[folds == i_fold]\n",
    "        \n",
    "        ### TRAINING\n",
    "        # For Toeplitz LDA make sure the order is [n_trials, epochs, features, channels]\n",
    "        X_trn_trials = np.transpose(X_trn_trials, (0, 1, 3, 2))\n",
    "    \n",
    "        \n",
    "        # Flatten training trials into epochs: shape becomes [n_trials * epochs, features * channels]\n",
    "        X_trn_epochs = X_trn_trials.reshape(-1, X_trn_trials.shape[2] * X_trn_trials.shape[3])\n",
    "        \n",
    "        # Extract labels for training epochs using z and y \n",
    "        trial_indices_trn = np.arange(len(y_trn_trials))  # indices for trials\n",
    "        y_trn_epochs = z_trn_trials[trial_indices_trn, :, y_trn_trials].reshape(-1)\n",
    "        \n",
    "        # Filter training epochs\n",
    "        X_trn_epochs, y_trn_epochs, _ = filter_valid_epochs(X_trn_epochs, y_trn_epochs)\n",
    "        \n",
    "        # Balance training classes\n",
    "        if do_balance_classes:\n",
    "            X_trn_epochs, y_trn_epochs = balance_classes(X_trn_epochs, y_trn_epochs, ratio_0_to_1=ratio_0_to_1)\n",
    "        \n",
    "        # Sanity check for training data\n",
    "        if np.isnan(X_trn_epochs).any() or np.isnan(y_trn_epochs).any():\n",
    "            raise ValueError(\"NaNs found in training epochs after filtering!\")\n",
    "        \n",
    "        ## Fit LDA\n",
    "        toeplitz = ToeplitzLDA(n_channels=n_channels)\n",
    "\n",
    "        # Dimensionality bust be 2\n",
    "        toeplitz.fit(X_trn_epochs, y_trn_epochs)\n",
    "\n",
    "        ### TESTING\n",
    "        # For Toeplitz LDA make sure the order is [n_trials * epochs, features * channels]\n",
    "        X_tst_trials = np.transpose(X_tst_trials, (0, 1, 3, 2))                                    \n",
    "                                    \n",
    "        # Flatten testing trials into epochs\n",
    "        X_tst_epochs = X_tst_trials.reshape(-1, X_tst_trials.shape[2] * X_tst_trials.shape[3])\n",
    "        \n",
    "        # Extract labels for testing epochs (again, using z and y for indexing)\n",
    "        trial_indices_tst = np.arange(len(y_tst_trials))\n",
    "        y_tst_epochs = z_tst_trials[trial_indices_tst, :, y_tst_trials].reshape(-1)\n",
    "        \n",
    "        # Reshape z_tst_trials into epochs.\n",
    "        z_tst_epochs = z_tst_trials.reshape(len(y_tst_trials) * z_tst_trials.shape[1], 2)\n",
    "        \n",
    "        # Filter testing epochs and also retrieve the original mask so we can count epochs per trial\n",
    "        X_tst_epochs, y_tst_epochs, z_tst_epochs, combined_mask_tst = filter_valid_epochs(\n",
    "            X_tst_epochs, y_tst_epochs, z=z_tst_epochs, return_mask=True\n",
    "        )\n",
    "        \n",
    "        # Calculate the number of preserved epochs per trial for testing.\n",
    "        # Here, combined_mask_tst still has the original shape before filtering.\n",
    "        # Reshape it to [n_trials, epochs_per_trial] and sum True values per trial.\n",
    "        epoch_counts = combined_mask_tst.reshape(len(y_tst_trials), -1)\n",
    "        num_epochs = np.sum(epoch_counts, axis=1)\n",
    "        \n",
    "        # Rebuild trial structure for testing data based on num_epochs\n",
    "        nested_X_tst_trials = []\n",
    "        nested_z_tst_epochs = []\n",
    "        start_idx = 0\n",
    "        for trial_idx, n_ep in enumerate(num_epochs):\n",
    "            end_idx = start_idx + n_ep\n",
    "            nested_X_tst_trials.append(X_tst_epochs[start_idx:end_idx])\n",
    "            nested_z_tst_epochs.append(z_tst_epochs[start_idx:end_idx])\n",
    "            start_idx = end_idx\n",
    "        \n",
    "\n",
    "        # Evaluate model on test data\n",
    "        correct_trials = 0\n",
    "        \n",
    "        for t_idx in range(len(y_tst_trials)):\n",
    "\n",
    "            num_preserved_epochs = num_epochs[t_idx]\n",
    "            if num_preserved_epochs < discard_threshold:\n",
    "                discarded_trial_counter +=1\n",
    "                continue\n",
    "            \n",
    "            # Log cued side informed by y_tst\n",
    "            cued_side = y_tst_trials[t_idx]\n",
    "            # create event vectors & ground truth\n",
    "            left_targets = nested_z_tst_epochs[t_idx][:, 0]\n",
    "            right_targets = nested_z_tst_epochs[t_idx][:, 1]\n",
    "            cued_targets = nested_z_tst_epochs[t_idx] [:, cued_side]\n",
    "   \n",
    "            # Compute LDA scores for epochs\n",
    "            epoch_scores = toeplitz.decision_function(nested_X_tst_trials[t_idx])\n",
    "            \n",
    "            # Log performance per fold\n",
    "            precision, recall, _ = precision_recall_curve(cued_targets, epoch_scores)\n",
    "            pr_auc_score = auc(recall, precision)\n",
    "            fold_pr_auc.append(pr_auc_score)\n",
    "\n",
    "            # Correlation-based decision\n",
    "            corr_left, _ = pearsonr(epoch_scores, left_targets)\n",
    "            corr_right, _ = pearsonr(epoch_scores, right_targets)\n",
    "\n",
    "            # Triel-level decision rule based on correlation\n",
    "            decision = 0 if corr_left > corr_right else 1\n",
    "            if decision == cued_side:\n",
    "                correct_trials += 1\n",
    "        \n",
    "        fold_correct_trials.append(correct_trials)\n",
    "    discarded_trials.append([subject, discarded_trial_counter/ n_folds])\n",
    "    # Compute perf metrics\n",
    "    pr_auc_mean = np.mean(fold_pr_auc)\n",
    "    pr_auc_se = np.std(fold_pr_auc, ddof=1) / np.sqrt(len(fold_pr_auc))\n",
    "    correct_trials_mean = np.mean(fold_correct_trials) / (y.size / n_folds)\n",
    "    correct_trials_se = np.std(fold_correct_trials, ddof=1) / np.sqrt(len(fold_correct_trials)) / (y.size / n_folds)\n",
    "    print(\"Accuracy:\", correct_trials_mean)\n",
    "        \n",
    "     # --- Prepare Data for saving ---\n",
    "    # Store results\n",
    "    results_decoding[\"subjects\"].append({\n",
    "        \"subject_id\": subject,\n",
    "        \"pr_auc_mean\": pr_auc_mean,\n",
    "        \"pr_auc_se\": pr_auc_se,\n",
    "        \"correct_trials_mean\": correct_trials_mean,\n",
    "        \"correct_trials_se\": correct_trials_se\n",
    "    })\n",
    "    # --- Add Aggregate (Overall) Results Before Saving ---\n",
    "pr_auc_means = [subj['pr_auc_mean'] for subj in results_decoding[\"subjects\"]]\n",
    "correct_trials_means = [subj['correct_trials_mean'] for subj in results_decoding[\"subjects\"]]\n",
    "\n",
    "overall_result = {\n",
    "    \"subject_id\": \"Overall\",\n",
    "    \"pr_auc_mean\": np.mean(pr_auc_means),\n",
    "    \"pr_auc_se\": np.std(pr_auc_means, ddof=1) / np.sqrt(len(pr_auc_means)),\n",
    "    \"correct_trials_mean\": np.mean(correct_trials_means),\n",
    "    \"correct_trials_se\": np.std(correct_trials_means, ddof=1) / np.sqrt(len(correct_trials_means))\n",
    "}\n",
    "results_decoding[\"subjects\"].append(overall_result)\n",
    "\n",
    "# Convert results to a structured NumPy array for saving\n",
    "dtype = [\n",
    "    ('subject_id', 'U10'),   # Unicode string (max length 10)\n",
    "    ('pr_auc_mean', 'f8'),\n",
    "    ('pr_auc_se', 'f8'),\n",
    "    ('correct_trials_mean', 'f8'),\n",
    "    ('correct_trials_se', 'f8')\n",
    "]\n",
    "structured_array = np.array(\n",
    "    [\n",
    "        (\n",
    "            subj['subject_id'],\n",
    "            subj['pr_auc_mean'],\n",
    "            subj['pr_auc_se'],\n",
    "            subj['correct_trials_mean'],\n",
    "            subj['correct_trials_se']\n",
    "        )\n",
    "        for subj in results_decoding['subjects']\n",
    "    ],\n",
    "    dtype=dtype\n",
    ")\n",
    "\n",
    "# Save the structured array\n",
    "if not os.path.exists(decoding_results_dir):\n",
    "    os.makedirs(decoding_results_dir)\n",
    "np.save(join(decoding_results_dir, f\"{task}_toeplitzLDA_p300_ICA_results.npy\"), structured_array)\n",
    "\n",
    "# --- Print Overall Metrics ---\n",
    "overall_pr_auc_mean = overall_result['pr_auc_mean']\n",
    "overall_pr_auc_se = overall_result['pr_auc_se']\n",
    "overall_accuracy_mean = overall_result['correct_trials_mean']\n",
    "overall_accuracy_se = overall_result['correct_trials_se']\n",
    "\n",
    "print(f\"Overall LDA PR-AUC: {overall_pr_auc_mean:.2f} ± {overall_pr_auc_se:.2f}\")\n",
    "print(f\"Overall trial level inference accuracy: {overall_accuracy_mean:.2f} ± {overall_accuracy_se:.2f}\")\n",
    "\n",
    "# --- Log Discarded Trials ---\n",
    "\n",
    "discarded_trials_log = [(subj, val) for subj, val in discarded_trials if val > (1 / 4)] # account for fold-average\n",
    "if discarded_trials_log:\n",
    "    print(f\"Trials discarded (< {discard_threshold} epochs available for integration) for the following subjects (fold-average):\")\n",
    "    for subj, val in discarded_trials_log:\n",
    "        print(f\"Subject: {subj}, Discarded Trials: {val}\")\n",
    "else:\n",
    "    print(\"No trials discarded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450537ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
