{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2fad454",
   "metadata": {},
   "source": [
    "# Notebook for hybrid decoding\n",
    "Below are the pipelines used for the hybrid decoding. It first extracts the features for each pipeline individually, then, it concatenates these features and feeds this into an LDA. The feature extraction for c-VEP and alpha are directly from previous notebooks, however, for in order to get the P300 features in the correct format to concatenate it with the others, some adjustments had to be made.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0712f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pyntbci\n",
    "from scipy.signal import butter, sosfilt, hilbert\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from mne.decoding import CSP\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b40ab",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "Below are the cells which are used to extract the features for c-VEP and alpha, respectively. The feature extraction for P300 was done in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c062bf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VPpdia\tovert: (20, 2)\n",
      "saved to /Users/juliette/Desktop/thesis/features/c-VEP/sub-VPpdia_task-overt_features.npz, shape: (20, 2)\n",
      "covert: (80, 2)\n",
      "saved to /Users/juliette/Desktop/thesis/features/c-VEP/sub-VPpdia_task-covert_features.npz, shape: (80, 2)\n",
      "VPpdib\tovert: (20, 2)\n",
      "saved to /Users/juliette/Desktop/thesis/features/c-VEP/sub-VPpdib_task-overt_features.npz, shape: (20, 2)\n",
      "covert: (80, 2)\n",
      "saved to /Users/juliette/Desktop/thesis/features/c-VEP/sub-VPpdib_task-covert_features.npz, shape: (80, 2)\n",
      "VPpdic\tovert: (20, 2)\n",
      "saved to /Users/juliette/Desktop/thesis/features/c-VEP/sub-VPpdic_task-overt_features.npz, shape: (20, 2)\n",
      "covert: (80, 2)\n",
      "saved to /Users/juliette/Desktop/thesis/features/c-VEP/sub-VPpdic_task-covert_features.npz, shape: (80, 2)\n",
      "VPpdid\tovert: (20, 2)\n",
      "saved to /Users/juliette/Desktop/thesis/features/c-VEP/sub-VPpdid_task-overt_features.npz, shape: (20, 2)\n",
      "covert: (80, 2)\n",
      "saved to /Users/juliette/Desktop/thesis/features/c-VEP/sub-VPpdid_task-covert_features.npz, shape: (80, 2)\n",
      "VPpdie\tovert: (20, 2)\n",
      "saved to /Users/juliette/Desktop/thesis/features/c-VEP/sub-VPpdie_task-overt_features.npz, shape: (20, 2)\n",
      "covert: (80, 2)\n",
      "saved to /Users/juliette/Desktop/thesis/features/c-VEP/sub-VPpdie_task-covert_features.npz, shape: (80, 2)\n",
      "VPpdif\tovert: (20, 2)\n",
      "saved to /Users/juliette/Desktop/thesis/features/c-VEP/sub-VPpdif_task-overt_features.npz, shape: (20, 2)\n",
      "covert: (80, 2)\n",
      "saved to /Users/juliette/Desktop/thesis/features/c-VEP/sub-VPpdif_task-covert_features.npz, shape: (80, 2)\n",
      "VPpdig\tovert: (20, 2)\n",
      "saved to /Users/juliette/Desktop/thesis/features/c-VEP/sub-VPpdig_task-overt_features.npz, shape: (20, 2)\n",
      "covert: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Fit RCCA model\u001b[39;00m\n\u001b[1;32m     46\u001b[0m rcca \u001b[38;5;241m=\u001b[39m pyntbci\u001b[38;5;241m.\u001b[39mclassifiers\u001b[38;5;241m.\u001b[39mrCCA(stimulus\u001b[38;5;241m=\u001b[39mV, fs\u001b[38;5;241m=\u001b[39mfs, event\u001b[38;5;241m=\u001b[39mevent,\n\u001b[1;32m     47\u001b[0m                                 encoding_length\u001b[38;5;241m=\u001b[39mencoding_length,\n\u001b[1;32m     48\u001b[0m                                 onset_event\u001b[38;5;241m=\u001b[39monset_event, ensemble\u001b[38;5;241m=\u001b[39mensemble)\n\u001b[0;32m---> 49\u001b[0m \u001b[43mrcca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Extract features (project trials into canonical space)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m features \u001b[38;5;241m=\u001b[39m rcca\u001b[38;5;241m.\u001b[39mdecision_function(X)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/pyntbci/classifiers.py:846\u001b[0m, in \u001b[0;36mrCCA.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_class \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_classes):\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cca\u001b[38;5;241m.\u001b[39mappend(CCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components, gamma_x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma_x, gamma_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma_m,\n\u001b[1;32m    845\u001b[0m                          estimator_x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov_estimator_x, estimator_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov_estimator_m))\n\u001b[0;32m--> 846\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cca\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_class\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_class\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi_class\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_[:, :, i_class] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cca[i_class]\u001b[38;5;241m.\u001b[39mw_x_\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr_[:, :, i_class] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cca[i_class]\u001b[38;5;241m.\u001b[39mw_y_\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/pyntbci/transformers.py:203\u001b[0m, in \u001b[0;36mCCA.fit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X3D_Y1D(X, Y)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m Y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X3D_Y3D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m Y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X2D_Y2D(X, Y)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/pyntbci/transformers.py:154\u001b[0m, in \u001b[0;36mCCA._fit_X3D_Y3D\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    151\u001b[0m Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape((n_samples \u001b[38;5;241m*\u001b[39m n_trials, n_features_y))\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# CCA\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X2D_Y2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/pyntbci/transformers.py:74\u001b[0m, in \u001b[0;36mCCA._fit_X2D_Y2D\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Compute covariances\u001b[39;00m\n\u001b[1;32m     73\u001b[0m Z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X, Y), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_xy_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_xy_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov_xy_ \u001b[38;5;241m=\u001b[39m \u001b[43mcovariance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_xy_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_xy_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov_xy_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_x_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_xy_\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/pyntbci/utilities.py:106\u001b[0m, in \u001b[0;36mcovariance\u001b[0;34m(X, n_old, avg_old, cov_old, estimator, running)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the covariance matrix.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    The covariance of shape (n_features, n_features).\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m n_obs \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 106\u001b[0m avg_obs \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_old \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m running:\n\u001b[1;32m    108\u001b[0m     n_new \u001b[38;5;241m=\u001b[39m n_obs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3505\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/numpy/core/_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    115\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    116\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -- c-VEP feature extraction --\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Extract c-VEP RCCA features per trial per subject and task.\n",
    "@author: Adapted from Jordy Thielen\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Paths\n",
    "data_dir = '/Users/juliette/Desktop/thesis/preprocessing/c-VEP_preprocessing/c-VEP_ICA'\n",
    "save_dir = '/Users/juliette/Desktop/thesis/features/c-VEP'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Subject and task lists\n",
    "subjects = [\n",
    "    \"VPpdia\", \"VPpdib\", \"VPpdic\", \"VPpdid\", \"VPpdie\", \"VPpdif\", \"VPpdig\", \"VPpdih\", \"VPpdii\", \"VPpdij\", \"VPpdik\",\n",
    "    \"VPpdil\", \"VPpdim\", \"VPpdin\", \"VPpdio\", \"VPpdip\", \"VPpdiq\", \"VPpdir\", \"VPpdis\", \"VPpdit\", \"VPpdiu\", \"VPpdiv\",\n",
    "    \"VPpdiw\", \"VPpdix\", \"VPpdiy\", \"VPpdiz\", \"VPpdiza\", \"VPpdizb\", \"VPpdizc\"\n",
    "]\n",
    "tasks = [\"overt\", \"covert\"]\n",
    "\n",
    "# RCCA settings\n",
    "event = \"dur\"\n",
    "onset_event = True\n",
    "encoding_length = 0.3\n",
    "ensemble = True\n",
    "\n",
    "# Loop participants\n",
    "for subject in subjects:\n",
    "    print(f\"{subject}\", end=\"\\t\")\n",
    "\n",
    "    for task in tasks:\n",
    "        print(f\"{task}: \", end=\"\")\n",
    "\n",
    "        # Load data\n",
    "        fn = os.path.join(data_dir, f\"sub-{subject}_task-{task}_ICA.npz\")\n",
    "        tmp = np.load(fn)\n",
    "        fs = int(tmp[\"fs\"])\n",
    "        X = tmp[\"X\"]\n",
    "        y = tmp[\"y\"]\n",
    "        V = tmp[\"V\"]\n",
    "\n",
    "        # Fit RCCA model\n",
    "        rcca = pyntbci.classifiers.rCCA(stimulus=V, fs=fs, event=event,\n",
    "                                        encoding_length=encoding_length,\n",
    "                                        onset_event=onset_event, ensemble=ensemble)\n",
    "        rcca.fit(X, y)\n",
    "\n",
    "        # Extract features (project trials into canonical space)\n",
    "        features = rcca.decision_function(X)\n",
    "        print(features.shape)\n",
    "        features = features.reshape(features.shape[0], -1)  # shape: (n_trials, n_classes * n_components)\n",
    "\n",
    "        save_path = os.path.join(save_dir, f\"sub-{subject}_task-{task}_features.npz\")\n",
    "        np.savez(save_path, features=features, y=y)\n",
    "        print(f\"saved to {save_path}, shape: {features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c518da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subject VPpdia\n",
      "Shape of X: (80, 62, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 62 dim * 9.3e+09  max singular value)\n",
      "    Estimated rank (mag): 62\n",
      "    MAG: rank 62 computed from 62 data channels with 0 projectors\n",
      "Reducing data rank from 62 -> 62\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 62 dim * 9.5e+09  max singular value)\n",
      "    Estimated rank (mag): 62\n",
      "    MAG: rank 62 computed from 62 data channels with 0 projectors\n",
      "Reducing data rank from 62 -> 62\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdia: (80, 4)\n",
      "Saved features for subject VPpdia to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdia_task-covert_alpha_features.npz\n",
      "Processing subject VPpdib\n",
      "Shape of X: (80, 61, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00051 (2.2e-16 eps * 61 dim * 3.8e+10  max singular value)\n",
      "    Estimated rank (mag): 61\n",
      "    MAG: rank 61 computed from 61 data channels with 0 projectors\n",
      "Reducing data rank from 61 -> 61\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00045 (2.2e-16 eps * 61 dim * 3.4e+10  max singular value)\n",
      "    Estimated rank (mag): 61\n",
      "    MAG: rank 61 computed from 61 data channels with 0 projectors\n",
      "Reducing data rank from 61 -> 61\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdib: (80, 4)\n",
      "Saved features for subject VPpdib to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdib_task-covert_alpha_features.npz\n",
      "Processing subject VPpdic\n",
      "Shape of X: (80, 63, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00017 (2.2e-16 eps * 63 dim * 1.2e+10  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 63 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdic: (80, 4)\n",
      "Saved features for subject VPpdic to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdic_task-covert_alpha_features.npz\n",
      "Processing subject VPpdid\n",
      "Shape of X: (80, 63, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00029 (2.2e-16 eps * 63 dim * 2.1e+10  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0003 (2.2e-16 eps * 63 dim * 2.1e+10  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdid: (80, 4)\n",
      "Saved features for subject VPpdid to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdid_task-covert_alpha_features.npz\n",
      "Processing subject VPpdie\n",
      "Shape of X: (80, 63, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00011 (2.2e-16 eps * 63 dim * 8.2e+09  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00011 (2.2e-16 eps * 63 dim * 8e+09  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdie: (80, 4)\n",
      "Saved features for subject VPpdie to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdie_task-covert_alpha_features.npz\n",
      "Processing subject VPpdif\n",
      "Shape of X: (80, 62, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00033 (2.2e-16 eps * 62 dim * 2.4e+10  max singular value)\n",
      "    Estimated rank (mag): 62\n",
      "    MAG: rank 62 computed from 62 data channels with 0 projectors\n",
      "Reducing data rank from 62 -> 62\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00028 (2.2e-16 eps * 62 dim * 2.1e+10  max singular value)\n",
      "    Estimated rank (mag): 62\n",
      "    MAG: rank 62 computed from 62 data channels with 0 projectors\n",
      "Reducing data rank from 62 -> 62\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdif: (80, 4)\n",
      "Saved features for subject VPpdif to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdif_task-covert_alpha_features.npz\n",
      "Processing subject VPpdig\n",
      "Shape of X: (80, 63, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.1e-05 (2.2e-16 eps * 63 dim * 5.8e+09  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.1e-05 (2.2e-16 eps * 63 dim * 5.8e+09  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdig: (80, 4)\n",
      "Saved features for subject VPpdig to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdig_task-covert_alpha_features.npz\n",
      "Processing subject VPpdih\n",
      "Shape of X: (80, 62, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00043 (2.2e-16 eps * 62 dim * 3.2e+10  max singular value)\n",
      "    Estimated rank (mag): 62\n",
      "    MAG: rank 62 computed from 62 data channels with 0 projectors\n",
      "Reducing data rank from 62 -> 62\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00039 (2.2e-16 eps * 62 dim * 2.8e+10  max singular value)\n",
      "    Estimated rank (mag): 62\n",
      "    MAG: rank 62 computed from 62 data channels with 0 projectors\n",
      "Reducing data rank from 62 -> 62\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdih: (80, 4)\n",
      "Saved features for subject VPpdih to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdih_task-covert_alpha_features.npz\n",
      "Processing subject VPpdii\n",
      "Shape of X: (80, 63, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00034 (2.2e-16 eps * 63 dim * 2.4e+10  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00032 (2.2e-16 eps * 63 dim * 2.3e+10  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdii: (80, 4)\n",
      "Saved features for subject VPpdii to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdii_task-covert_alpha_features.npz\n",
      "Processing subject VPpdij\n",
      "Shape of X: (80, 62, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 62 dim * 8.9e+09  max singular value)\n",
      "    Estimated rank (mag): 62\n",
      "    MAG: rank 62 computed from 62 data channels with 0 projectors\n",
      "Reducing data rank from 62 -> 62\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 62 dim * 8.8e+09  max singular value)\n",
      "    Estimated rank (mag): 62\n",
      "    MAG: rank 62 computed from 62 data channels with 0 projectors\n",
      "Reducing data rank from 62 -> 62\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdij: (80, 4)\n",
      "Saved features for subject VPpdij to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdij_task-covert_alpha_features.npz\n",
      "Processing subject VPpdik\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (80, 63, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 63 dim * 9.4e+09  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 63 dim * 9.3e+09  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdik: (80, 4)\n",
      "Saved features for subject VPpdik to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdik_task-covert_alpha_features.npz\n",
      "Processing subject VPpdil\n",
      "Shape of X: (80, 63, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 63 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00018 (2.2e-16 eps * 63 dim * 1.3e+10  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdil: (80, 4)\n",
      "Saved features for subject VPpdil to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdil_task-covert_alpha_features.npz\n",
      "Processing subject VPpdim\n",
      "Shape of X: (80, 64, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00038 (2.2e-16 eps * 64 dim * 2.7e+10  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00033 (2.2e-16 eps * 64 dim * 2.3e+10  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdim: (80, 4)\n",
      "Saved features for subject VPpdim to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdim_task-covert_alpha_features.npz\n",
      "Processing subject VPpdin\n",
      "Shape of X: (80, 62, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 62 dim * 7.5e+09  max singular value)\n",
      "    Estimated rank (mag): 62\n",
      "    MAG: rank 62 computed from 62 data channels with 0 projectors\n",
      "Reducing data rank from 62 -> 62\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.8e-05 (2.2e-16 eps * 62 dim * 7.1e+09  max singular value)\n",
      "    Estimated rank (mag): 62\n",
      "    MAG: rank 62 computed from 62 data channels with 0 projectors\n",
      "Reducing data rank from 62 -> 62\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdin: (80, 4)\n",
      "Saved features for subject VPpdin to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdin_task-covert_alpha_features.npz\n",
      "Processing subject VPpdio\n",
      "Shape of X: (80, 64, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00015 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00014 (2.2e-16 eps * 64 dim * 1e+10  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdio: (80, 4)\n",
      "Saved features for subject VPpdio to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdio_task-covert_alpha_features.npz\n",
      "Processing subject VPpdip\n",
      "Shape of X: (80, 64, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00021 (2.2e-16 eps * 64 dim * 1.5e+10  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0002 (2.2e-16 eps * 64 dim * 1.4e+10  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdip: (80, 4)\n",
      "Saved features for subject VPpdip to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdip_task-covert_alpha_features.npz\n",
      "Processing subject VPpdiq\n",
      "Shape of X: (80, 64, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00031 (2.2e-16 eps * 64 dim * 2.2e+10  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00028 (2.2e-16 eps * 64 dim * 2e+10  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdiq: (80, 4)\n",
      "Saved features for subject VPpdiq to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdiq_task-covert_alpha_features.npz\n",
      "Processing subject VPpdir\n",
      "Shape of X: (80, 64, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00021 (2.2e-16 eps * 64 dim * 1.5e+10  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00021 (2.2e-16 eps * 64 dim * 1.5e+10  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdir: (80, 4)\n",
      "Saved features for subject VPpdir to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdir_task-covert_alpha_features.npz\n",
      "Processing subject VPpdis\n",
      "Shape of X: (80, 64, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00032 (2.2e-16 eps * 64 dim * 2.3e+10  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0003 (2.2e-16 eps * 64 dim * 2.1e+10  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdis: (80, 4)\n",
      "Saved features for subject VPpdis to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdis_task-covert_alpha_features.npz\n",
      "Processing subject VPpdit\n",
      "Shape of X: (80, 64, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00042 (2.2e-16 eps * 64 dim * 3e+10  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00038 (2.2e-16 eps * 64 dim * 2.7e+10  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdit: (80, 4)\n",
      "Saved features for subject VPpdit to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdit_task-covert_alpha_features.npz\n",
      "Processing subject VPpdiu\n",
      "Shape of X: (80, 63, 10239)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00011 (2.2e-16 eps * 63 dim * 7.9e+09  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00011 (2.2e-16 eps * 63 dim * 8.1e+09  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdiu: (80, 4)\n",
      "Saved features for subject VPpdiu to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdiu_task-covert_alpha_features.npz\n",
      "Processing subject VPpdiv\n",
      "Shape of X: (80, 60, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00034 (2.2e-16 eps * 60 dim * 2.6e+10  max singular value)\n",
      "    Estimated rank (mag): 60\n",
      "    MAG: rank 60 computed from 60 data channels with 0 projectors\n",
      "Reducing data rank from 60 -> 60\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00032 (2.2e-16 eps * 60 dim * 2.4e+10  max singular value)\n",
      "    Estimated rank (mag): 60\n",
      "    MAG: rank 60 computed from 60 data channels with 0 projectors\n",
      "Reducing data rank from 60 -> 60\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdiv: (80, 4)\n",
      "Saved features for subject VPpdiv to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdiv_task-covert_alpha_features.npz\n",
      "Processing subject VPpdiw\n",
      "Shape of X: (80, 59, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 59 dim * 9.8e+09  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 59 dim * 1e+10  max singular value)\n",
      "    Estimated rank (mag): 59\n",
      "    MAG: rank 59 computed from 59 data channels with 0 projectors\n",
      "Reducing data rank from 59 -> 59\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdiw: (80, 4)\n",
      "Saved features for subject VPpdiw to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdiw_task-covert_alpha_features.npz\n",
      "Processing subject VPpdix\n",
      "Shape of X: (80, 64, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00076 (2.2e-16 eps * 64 dim * 5.4e+10  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00072 (2.2e-16 eps * 64 dim * 5.1e+10  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdix: (80, 4)\n",
      "Saved features for subject VPpdix to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdix_task-covert_alpha_features.npz\n",
      "Processing subject VPpdiy\n",
      "Shape of X: (80, 64, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.9e-05 (2.2e-16 eps * 64 dim * 6.9e+09  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.5e-05 (2.2e-16 eps * 64 dim * 6.7e+09  max singular value)\n",
      "    Estimated rank (mag): 64\n",
      "    MAG: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdiy: (80, 4)\n",
      "Saved features for subject VPpdiy to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdiy_task-covert_alpha_features.npz\n",
      "Processing subject VPpdiz\n",
      "Shape of X: (80, 62, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0002 (2.2e-16 eps * 62 dim * 1.5e+10  max singular value)\n",
      "    Estimated rank (mag): 62\n",
      "    MAG: rank 62 computed from 62 data channels with 0 projectors\n",
      "Reducing data rank from 62 -> 62\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00019 (2.2e-16 eps * 62 dim * 1.4e+10  max singular value)\n",
      "    Estimated rank (mag): 62\n",
      "    MAG: rank 62 computed from 62 data channels with 0 projectors\n",
      "Reducing data rank from 62 -> 62\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdiz: (80, 4)\n",
      "Saved features for subject VPpdiz to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdiz_task-covert_alpha_features.npz\n",
      "Processing subject VPpdiza\n",
      "Shape of X: (80, 63, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0003 (2.2e-16 eps * 63 dim * 2.2e+10  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00029 (2.2e-16 eps * 63 dim * 2.1e+10  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdiza: (80, 4)\n",
      "Saved features for subject VPpdiza to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdiza_task-covert_alpha_features.npz\n",
      "Processing subject VPpdizb\n",
      "Shape of X: (80, 63, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 63 dim * 9.3e+09  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 63 dim * 9.2e+09  max singular value)\n",
      "    Estimated rank (mag): 63\n",
      "    MAG: rank 63 computed from 63 data channels with 0 projectors\n",
      "Reducing data rank from 63 -> 63\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdizb: (80, 4)\n",
      "Saved features for subject VPpdizb to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdizb_task-covert_alpha_features.npz\n",
      "Processing subject VPpdizc\n",
      "Shape of X: (80, 58, 10239)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.1e-05 (2.2e-16 eps * 58 dim * 5.5e+09  max singular value)\n",
      "    Estimated rank (mag): 58\n",
      "    MAG: rank 58 computed from 58 data channels with 0 projectors\n",
      "Reducing data rank from 58 -> 58\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7e-05 (2.2e-16 eps * 58 dim * 5.5e+09  max singular value)\n",
      "    Estimated rank (mag): 58\n",
      "    MAG: rank 58 computed from 58 data channels with 0 projectors\n",
      "Reducing data rank from 58 -> 58\n",
      "Estimating covariance using SHRINKAGE\n",
      "Done.\n",
      "Extracted features shape for subject VPpdizc: (80, 4)\n",
      "Saved features for subject VPpdizc to /Users/juliette/Desktop/thesis/features/alpha/sub-VPpdizc_task-covert_alpha_features.npz\n"
     ]
    }
   ],
   "source": [
    "# Bandpass filter function\n",
    "def bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
    "    \"\"\"\n",
    "    Apply a bandpass filter to the data.\n",
    "    \"\"\"\n",
    "    sos = butter(order, [lowcut, highcut], btype='band', fs=fs, output='sos')\n",
    "    return sosfilt(sos, data, axis=-1)\n",
    "\n",
    "# Hilbert transform feature function\n",
    "def compute_average_hilbert_amplitude(data):\n",
    "    \"\"\"\n",
    "    Compute log-mean amplitude using Hilbert transform.\n",
    "    \"\"\"\n",
    "    analytic = hilbert(data, axis=2)\n",
    "    amplitude = np.abs(analytic)\n",
    "    mean_amplitude = amplitude.mean(axis=2)\n",
    "    return np.log(mean_amplitude)\n",
    "\n",
    "# Parameters\n",
    "task = \"covert\"\n",
    "n_comp = 4  # number of CSP components\n",
    "subjects = [\n",
    "    \"VPpdia\", \"VPpdib\", \"VPpdic\", \"VPpdid\", \"VPpdie\", \"VPpdif\", \"VPpdig\", \"VPpdih\", \"VPpdii\", \"VPpdij\", \"VPpdik\",\n",
    "    \"VPpdil\", \"VPpdim\", \"VPpdin\", \"VPpdio\", \"VPpdip\", \"VPpdiq\", \"VPpdir\", \"VPpdis\", \"VPpdit\", \"VPpdiu\", \"VPpdiv\",\n",
    "    \"VPpdiw\", \"VPpdix\", \"VPpdiy\", \"VPpdiz\", \"VPpdiza\", \"VPpdizb\", \"VPpdizc\"\n",
    "]\n",
    "decoding_results_dir = '/Users/juliette/Desktop/thesis/features/alpha'\n",
    "\n",
    "# Loop through subjects\n",
    "for subject in subjects:\n",
    "    print(f\"Processing subject {subject}\")\n",
    "    \n",
    "    # File paths\n",
    "    file_dir = '/Users/juliette/Desktop/thesis/preprocessing/alpha_preprocessing/alpha_ICA'\n",
    "    file_path = os.path.join(file_dir, f\"sub-{subject}_task-{task}_alpha_ICA.npz\")\n",
    "\n",
    "    # Check file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Load data\n",
    "    npz_data = np.load(file_path)\n",
    "    X = npz_data['X']  # EEG data: trials x channels x samples\n",
    "    y = npz_data['y']  # Labels: trials\n",
    "    fs = npz_data['fs'].flatten()[0]  # Sampling frequency as integer\n",
    "\n",
    "    print(\"Shape of X:\", X.shape)\n",
    "\n",
    "    # Preprocessing\n",
    "    X = bandpass_filter(X, 8, 12, fs=fs)  # Alpha band filtering\n",
    "    X = X[:, :, 120:-120]  # Remove edge artifacts\n",
    "\n",
    "    # CSP and feature extraction\n",
    "    csp = CSP(n_components=n_comp, reg=0.01, log=None, transform_into='csp_space')\n",
    "    X_csp = csp.fit_transform(X, y)  # Apply CSP\n",
    "    features = compute_average_hilbert_amplitude(X_csp)  # Extract features\n",
    "\n",
    "    print(f\"Extracted features shape for subject {subject}: {features.shape}\")\n",
    "\n",
    "    # Save features\n",
    "    if not os.path.exists(decoding_results_dir):\n",
    "        os.makedirs(decoding_results_dir)\n",
    "\n",
    "    out_path = os.path.join(decoding_results_dir, f\"sub-{subject}_task-{task}_alpha_features.npz\")\n",
    "    np.savez(out_path, features=features, labels=y)\n",
    "\n",
    "    print(f\"Saved features for subject {subject} to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e94d9",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Below are the helper functions used for the P300 decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8afba0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(X, y, ratio_0_to_1=1.0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Sub-select X and y based on a specified ratio of 0s to 1s, keeping the original order.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "    y (numpy.ndarray): Label vector of shape (n_samples,).\n",
    "    ratio_0_to_1 (float): The desired ratio of 0s to 1s in the balanced dataset.\n",
    "\n",
    "    Returns:\n",
    "    X_balanced, y_balanced: Sub-selected feature matrix and label vector.\n",
    "    \"\"\"\n",
    "    # Step 1: Identify indices of 0s and 1s\n",
    "    indices_0 = np.where(y == 0)[0]\n",
    "    indices_1 = np.where(y == 1)[0]\n",
    "    \n",
    "    # Step 2: Calculate the number of samples to select for each class\n",
    "    num_1s = len(indices_1)\n",
    "    num_0s = min(len(indices_0), int(num_1s * ratio_0_to_1))\n",
    "    \n",
    "    # Step 3: Randomly sample the desired number of 0s and 1s\n",
    "    selected_indices_0 = np.random.choice(indices_0, num_0s, replace=False)\n",
    "    selected_indices_1 = np.random.choice(indices_1, num_1s, replace=False)\n",
    "    \n",
    "    # Step 4: Combine selected indices and sort to preserve original order\n",
    "    balanced_indices = np.sort(np.concatenate([selected_indices_0, selected_indices_1]))\n",
    "    \n",
    "    # Step 5: Sub-select X and y based on the balanced indices\n",
    "    X_balanced = X[balanced_indices]\n",
    "    y_balanced = y[balanced_indices]\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "def filter_valid_epochs(X, y, z=None, return_mask=False):\n",
    "    \"\"\"\n",
    "    Filters out epochs where either the features in X or the labels in y contain NaN values.\n",
    "    Optionally, if a z array is provided, it is filtered similarly.\n",
    "    \n",
    "    Parameters:\n",
    "        X (np.ndarray): A 2D numpy array with shape (n_epochs, n_features).\n",
    "        y (np.ndarray): A 1D numpy array with shape (n_epochs,).\n",
    "        z (np.ndarray, optional): An array that will be filtered using the same mask.\n",
    "        return_mask (bool, optional): If True, the boolean mask used for filtering is returned.\n",
    "    \n",
    "    Returns:\n",
    "        filtered_X (np.ndarray): X with only rows that have no NaN values.\n",
    "        filtered_y (np.ndarray): y with only entries corresponding to valid epochs.\n",
    "        filtered_z (np.ndarray or None): Filtered z array (if provided) or None.\n",
    "        mask (np.ndarray, optional): The boolean mask of valid epochs; only returned if return_mask=True.\n",
    "    \"\"\"\n",
    "    # Create a mask for valid labels and features\n",
    "    valid_label_mask = ~np.isnan(y)\n",
    "    valid_feature_mask = ~np.isnan(X).any(axis=1)\n",
    "    combined_mask = valid_label_mask & valid_feature_mask\n",
    "\n",
    "    # Apply the mask to X and y\n",
    "    filtered_X = X[combined_mask]\n",
    "    filtered_y = y[combined_mask]\n",
    "    \n",
    "    if z is not None:\n",
    "        filtered_z = z[combined_mask]\n",
    "    else:\n",
    "        filtered_z = None\n",
    "\n",
    "    if return_mask:\n",
    "        return filtered_X, filtered_y, filtered_z, combined_mask\n",
    "    else:\n",
    "        return filtered_X, filtered_y, filtered_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269fc0fe",
   "metadata": {},
   "source": [
    "# Hybrid decoding\n",
    "The cell below contains the decoding for P300, alpha and c-VEP together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed0fe73a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features for VPpdia, task=covert\n",
      "(80, 2)\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 62, 6)\n",
      "X_p300.shape: (80, 80, 62, 6)\n",
      "Shape of left_target_averaged_trials: (80, 62, 6)\n",
      "shape of X_combined: (80, 750)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.8\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.75\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.55\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.8\n",
      "Average accuracy over all folds: 0.7250000000000001\n",
      " STD: 0.10\n",
      "Loading features for VPpdib, task=covert\n",
      "(80, 2)\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 61, 6)\n",
      "X_p300.shape: (80, 80, 61, 6)\n",
      "Shape of left_target_averaged_trials: (80, 61, 6)\n",
      "shape of X_combined: (80, 738)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 121\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# LDA classifier\u001b[39;00m\n\u001b[1;32m    120\u001b[0m lda \u001b[38;5;241m=\u001b[39m LinearDiscriminantAnalysis(solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlsqr\u001b[39m\u001b[38;5;124m\"\u001b[39m, covariance_estimator\u001b[38;5;241m=\u001b[39mLedoitWolf())\n\u001b[0;32m--> 121\u001b[0m \u001b[43mlda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m    124\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m lda\u001b[38;5;241m.\u001b[39mpredict(X_tst)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:709\u001b[0m, in \u001b[0;36mLinearDiscriminantAnalysis.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solve_svd(X, y)\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlsqr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 709\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_solve_lstsq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshrinkage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinkage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcovariance_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariance_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meigen\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solve_eigen(\n\u001b[1;32m    717\u001b[0m         X,\n\u001b[1;32m    718\u001b[0m         y,\n\u001b[1;32m    719\u001b[0m         shrinkage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshrinkage,\n\u001b[1;32m    720\u001b[0m         covariance_estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_estimator,\n\u001b[1;32m    721\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:487\u001b[0m, in \u001b[0;36mLinearDiscriminantAnalysis._solve_lstsq\u001b[0;34m(self, X, y, shrinkage, covariance_estimator)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Least squares solver.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03mThe least squares solver computes a straightforward solution of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03m   0-471-05669-3.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans_ \u001b[38;5;241m=\u001b[39m _class_means(X, y)\n\u001b[0;32m--> 487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_ \u001b[38;5;241m=\u001b[39m \u001b[43m_class_cov\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpriors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshrinkage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovariance_estimator\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_ \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mlstsq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans_\u001b[38;5;241m.\u001b[39mT)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT)) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriors_\n\u001b[1;32m    493\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:167\u001b[0m, in \u001b[0;36m_class_cov\u001b[0;34m(X, y, priors, shrinkage, covariance_estimator)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes):\n\u001b[1;32m    166\u001b[0m     Xg \u001b[38;5;241m=\u001b[39m X[y \u001b[38;5;241m==\u001b[39m group, :]\n\u001b[0;32m--> 167\u001b[0m     cov \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m priors[idx] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_2d(\u001b[43m_cov\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshrinkage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovariance_estimator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cov\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:82\u001b[0m, in \u001b[0;36m_cov\u001b[0;34m(X, shrinkage, covariance_estimator)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shrinkage \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shrinkage \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovariance_estimator and shrinkage parameters \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mare not None. Only one of the two can be set.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m     )\n\u001b[0;32m---> 82\u001b[0m \u001b[43mcovariance_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(covariance_estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovariance_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m does not have a covariance_ attribute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;241m%\u001b[39m covariance_estimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m     87\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/covariance/_shrunk_covariance.py:610\u001b[0m, in \u001b[0;36mLedoitWolf.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    606\u001b[0m covariance, shrinkage \u001b[38;5;241m=\u001b[39m _ledoit_wolf(\n\u001b[1;32m    607\u001b[0m     X \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocation_, assume_centered\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, block_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_size\n\u001b[1;32m    608\u001b[0m )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshrinkage_ \u001b[38;5;241m=\u001b[39m shrinkage\n\u001b[0;32m--> 610\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_covariance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcovariance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/covariance/_empirical_covariance.py:215\u001b[0m, in \u001b[0;36mEmpiricalCovariance._set_covariance\u001b[0;34m(self, covariance)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# set precision\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_precision:\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_ \u001b[38;5;241m=\u001b[39m \u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpinvh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcovariance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/linalg/_basic.py:1542\u001b[0m, in \u001b[0;36mpinvh\u001b[0;34m(a, atol, rtol, lower, return_rank, check_finite)\u001b[0m\n\u001b[1;32m   1539\u001b[0m psigma_diag \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m s[above_cutoff]\n\u001b[1;32m   1540\u001b[0m u \u001b[38;5;241m=\u001b[39m u[:, above_cutoff]\n\u001b[0;32m-> 1542\u001b[0m B \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpsigma_diag\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_rank:\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m B, \u001b[38;5;28mlen\u001b[39m(psigma_diag)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "cvep_features_dir = '/Users/juliette/Desktop/thesis/features/c-VEP'  # where c-VEP features are stored\n",
    "alpha_features_dir = '/Users/juliette/Desktop/thesis/features/alpha'  # where alpha features are stored\n",
    "p300_features_dir = '/Users/juliette/Desktop/thesis/preprocessing/features/with_ICA'  # where P300 features are stored\n",
    "save_dir = '/Users/juliette/Desktop/thesis/results/hybrid_simple/alpha+p300+c-vep'\n",
    "\n",
    "# Subject and task lists\n",
    "subjects = [\n",
    "    \"VPpdia\", \"VPpdib\", \"VPpdic\", \"VPpdid\", \"VPpdie\", \"VPpdif\", \"VPpdig\", \"VPpdih\", \"VPpdii\", \"VPpdij\", \"VPpdik\",\n",
    "    \"VPpdil\", \"VPpdim\", \"VPpdin\", \"VPpdio\", \"VPpdip\", \"VPpdiq\", \"VPpdir\", \"VPpdis\", \"VPpdit\", \"VPpdiu\", \"VPpdiv\",\n",
    "    \"VPpdiw\", \"VPpdix\", \"VPpdiy\", \"VPpdiz\", \"VPpdiza\", \"VPpdizb\", \"VPpdizc\"\n",
    "]\n",
    "tasks = [\"covert\"]\n",
    "subject_accuracies = []\n",
    "subject_std = []\n",
    "\n",
    "# Loop over subjects and tasks\n",
    "for subject in subjects:\n",
    "    for task in tasks:\n",
    "        print(f\"Loading features for {subject}, task={task}\")\n",
    "\n",
    "        # Load c-VEP features\n",
    "        cvep_file_path = os.path.join(cvep_features_dir, f\"sub-{subject}_task-{task}_features.npz\")\n",
    "        if os.path.exists(cvep_file_path):\n",
    "            cvep_data = np.load(cvep_file_path)\n",
    "            X_cvep = cvep_data['features']  # Features for c-VEP task (trials x features)\n",
    "            print(X_cvep.shape)\n",
    "            y_cvep = cvep_data['y']  # Labels for c-VEP task\n",
    "            print(f\"Loaded c-VEP features: {X_cvep.shape}\")\n",
    "        else:\n",
    "            print(f\"Warning: c-VEP features file not found for {subject}, task={task}\")\n",
    "\n",
    "        # Load alpha features\n",
    "        alpha_file_path = os.path.join(alpha_features_dir, f\"sub-{subject}_task-{task}_alpha_features.npz\")\n",
    "        if os.path.exists(alpha_file_path):\n",
    "            alpha_data = np.load(alpha_file_path)\n",
    "            X_alpha = alpha_data['features']  # Features for alpha task (trials x features)\n",
    "            y_alpha = alpha_data['labels']  # Labels for alpha task\n",
    "            print(f\"Loaded alpha features: {X_alpha.shape}\")\n",
    "        else:\n",
    "            print(f\"Warning: alpha features file not found for {subject}, task={task}\")\n",
    "\n",
    "        # Load P300 features\n",
    "        file_path_p300 = os.path.join(p300_features_dir, f\"sub-{subject}\", f\"sub-{subject}_task-{task}_p300_features_ICA.npz\")\n",
    "        if os.path.exists(file_path_p300):\n",
    "            p300_features = np.load(file_path_p300)\n",
    "\n",
    "            X_p300 = p300_features['X']  # Shape: trials x epochs x channels x features\n",
    "            y_p300 = p300_features['y']  # Labels indicating cued side: trials\n",
    "            z_p300 = p300_features['z']  # Left and right targets: trials x epochs x sides\n",
    "            fs_p300 = p300_features['fs']\n",
    "            print(f\"Loaded p300 features: {X_p300.shape}\")\n",
    "\n",
    "        # Flatten z_p300 to (trials * epochs, sides)\n",
    "        z_p300_flat = z_p300.reshape(-1, z_p300.shape[2])  # Shape: (trials * epochs, sides)\n",
    "\n",
    "        # Find the number of epochs per trial\n",
    "        epochs_per_trial = X_p300.shape[1]  # Number of epochs per trial\n",
    "        print(\"X_p300.shape:\", X_p300.shape)\n",
    "\n",
    "        # Initialize lists to store averaged epochs for each trial\n",
    "        left_target_averaged_trials = []\n",
    "        right_target_averaged_trials = []\n",
    "\n",
    "        # Loop over each trial\n",
    "        for i_trial in range(X_p300.shape[0]):\n",
    "            # Extract the epochs for the current trial (shape: epochs x channels x features)\n",
    "            trial_epochs = X_p300[i_trial]\n",
    "\n",
    "            # Extract the target labels for the current trial (shape: epochs x 2)\n",
    "            trial_targets = z_p300_flat[i_trial * epochs_per_trial: (i_trial + 1) * epochs_per_trial]\n",
    "\n",
    "            # Average epochs for left target (assuming 1 = left target)\n",
    "            left_target_epochs = trial_epochs[trial_targets[:, 0] == 1]\n",
    "            if len(left_target_epochs) > 0:\n",
    "                left_target_averaged = np.mean(left_target_epochs, axis=0)  # Average across epochs (axis=0)\n",
    "            else:\n",
    "                left_target_averaged = np.zeros(trial_epochs.shape[1:])  # If no left target, set to zeros\n",
    "\n",
    "            # Average epochs for right target (assuming 1 = right target)\n",
    "            right_target_epochs = trial_epochs[trial_targets[:, 1] == 1]\n",
    "            if len(right_target_epochs) > 0:\n",
    "                right_target_averaged = np.mean(right_target_epochs, axis=0)  # Average across epochs (axis=0)\n",
    "            else:\n",
    "                right_target_averaged = np.zeros(trial_epochs.shape[1:])  # If no right target, set to zeros\n",
    "\n",
    "            # Store the averaged epochs for the current trial\n",
    "            left_target_averaged_trials.append(left_target_averaged)\n",
    "            right_target_averaged_trials.append(right_target_averaged)\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        left_target_averaged_trials = np.array(left_target_averaged_trials)\n",
    "        print(\"Shape of left_target_averaged_trials:\", left_target_averaged_trials.shape)\n",
    "        right_target_averaged_trials = np.array(right_target_averaged_trials)\n",
    "\n",
    "        # Concatenate the averaged features from both groups (left and right)\n",
    "        X_p300_averaged = np.concatenate([left_target_averaged_trials, right_target_averaged_trials], axis=1)\n",
    "\n",
    "        # Flatten the last two dimensions (channels and features)\n",
    "        X_p300_averaged_flat = X_p300_averaged.reshape(X_p300_averaged.shape[0], -1)\n",
    "        # Concatenate the features of the P300, alpha and c-VEP\n",
    "        X_combined = np.concatenate([X_p300_averaged_flat, X_alpha, X_cvep], axis=1)\n",
    "        print(\"shape of X_combined:\", X_combined.shape)\n",
    "\n",
    "        # Cross-validation setup\n",
    "        fold_accuracies = []\n",
    "        fold_roc_auc = []\n",
    "        n_folds = 4\n",
    "        n_trials = X_combined.shape[0] // n_folds\n",
    "        folds = np.repeat(np.arange(n_folds), n_trials)\n",
    "\n",
    "        for i_fold in range(n_folds):\n",
    "            print(f\"  Fold {i_fold + 1}/{n_folds}\")\n",
    "\n",
    "            # Split train and test data\n",
    "            X_trn, y_trn = X_combined[folds != i_fold, :], y_cvep[folds != i_fold]\n",
    "            X_tst, y_tst = X_combined[folds == i_fold, :], y_cvep[folds == i_fold]\n",
    "\n",
    "            # LDA classifier\n",
    "            lda = LinearDiscriminantAnalysis(solver=\"lsqr\", covariance_estimator=LedoitWolf())\n",
    "            lda.fit(X_trn, y_trn)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = lda.predict(X_tst)\n",
    "\n",
    "            # Compute performance metrics\n",
    "            accuracy = accuracy_score(y_tst, y_pred)\n",
    "            print(f\"Fold {i_fold+1} accuracy: {accuracy}\")\n",
    "\n",
    "            fold_accuracies.append(accuracy)\n",
    "\n",
    "        # Calculate average accuracy over all folds\n",
    "        average_accuracy = np.mean(fold_accuracies)\n",
    "        subject_accuracies.append(fold_accuracies)\n",
    "        subject_std.append(np.std(fold_accuracies))  # compute std over folds\n",
    "        print(f\"Average accuracy over all folds: {average_accuracy}\\n STD: {np.std(fold_accuracies):.2f}\")\n",
    "\n",
    "# grand_average_accuracy = np.mean(subject_accuracies)\n",
    "# print(f\"\\nGrand average accuracy across all subjects: {grand_average_accuracy:.4f}\")\n",
    "\n",
    "# # Convert list to array\n",
    "# subject_accuracies_array = np.array(subject_accuracies)\n",
    "# subject_std_array = np.array(subject_std)\n",
    "\n",
    "# # Save the results\n",
    "# save_path = os.path.join(save_dir, 'cvep_p300_alpha_hybrid_accuracy_results.npz')\n",
    "# np.savez(save_path,\n",
    "#          accuracy=subject_accuracies_array,\n",
    "#          std=subject_std_array,\n",
    "#          subjects=subjects,\n",
    "#          tasks=tasks,\n",
    "#          n_folds=n_folds,\n",
    "#          method='hybrid')\n",
    "\n",
    "# print(f\"\\nSaved results to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b1a79f",
   "metadata": {},
   "source": [
    "# Pairwise decoding\n",
    "Now the pairwise combinations are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c01fb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features for VPpdia, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.6\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.75\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.55\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.5\n",
      "Average accuracy over all folds: 0.6000000000000001\n",
      " STD: 0.09\n",
      "Loading features for VPpdib, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 1.0\n",
      " STD: 0.00\n",
      "Loading features for VPpdic, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.85\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.9625\n",
      " STD: 0.06\n",
      "Loading features for VPpdid, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.9\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.8\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.8\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.7\n",
      "Average accuracy over all folds: 0.8\n",
      " STD: 0.07\n",
      "Loading features for VPpdie, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.8\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.85\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.95\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.8999999999999999\n",
      " STD: 0.08\n",
      "Loading features for VPpdif, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.95\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.975\n",
      " STD: 0.03\n",
      "Loading features for VPpdig, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.95\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.95\n",
      "Average accuracy over all folds: 0.9624999999999999\n",
      " STD: 0.02\n",
      "Loading features for VPpdih, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.9\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.9625\n",
      " STD: 0.04\n",
      "Loading features for VPpdii, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.95\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.9875\n",
      " STD: 0.02\n",
      "Loading features for VPpdij, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.8\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.85\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.9\n",
      "Average accuracy over all folds: 0.875\n",
      " STD: 0.06\n",
      "Loading features for VPpdik, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.95\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.85\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.9\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.95\n",
      "Average accuracy over all folds: 0.9124999999999999\n",
      " STD: 0.04\n",
      "Loading features for VPpdil, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.9\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.975\n",
      " STD: 0.04\n",
      "Loading features for VPpdim, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 1.0\n",
      " STD: 0.00\n",
      "Loading features for VPpdin, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.9875\n",
      " STD: 0.02\n",
      "Loading features for VPpdio, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.95\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.9875\n",
      " STD: 0.02\n",
      "Loading features for VPpdip, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.95\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.9875\n",
      " STD: 0.02\n",
      "Loading features for VPpdiq, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.9\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.95\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.95\n",
      "Average accuracy over all folds: 0.9375\n",
      " STD: 0.02\n",
      "Loading features for VPpdir, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.95\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.95\n",
      "Average accuracy over all folds: 0.95\n",
      " STD: 0.04\n",
      "Loading features for VPpdis, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.975\n",
      " STD: 0.04\n",
      "Loading features for VPpdit, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 1.0\n",
      " STD: 0.00\n",
      "Loading features for VPpdiu, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.95\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.75\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.95\n",
      "Average accuracy over all folds: 0.8999999999999999\n",
      " STD: 0.09\n",
      "Loading features for VPpdiv, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.95\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.9875\n",
      " STD: 0.02\n",
      "Loading features for VPpdiw, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.95\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.9625\n",
      " STD: 0.04\n",
      "Loading features for VPpdix, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 1.0\n",
      " STD: 0.00\n",
      "Loading features for VPpdiy, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.95\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.95\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.95\n",
      "Average accuracy over all folds: 0.95\n",
      " STD: 0.00\n",
      "Loading features for VPpdiz, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.75\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.95\n",
      "Average accuracy over all folds: 0.8999999999999999\n",
      " STD: 0.09\n",
      "Loading features for VPpdiza, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.9\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.975\n",
      " STD: 0.04\n",
      "Loading features for VPpdizb, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.975\n",
      " STD: 0.04\n",
      "Loading features for VPpdizc, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.75\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.85\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.85\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.8\n",
      "Average accuracy over all folds: 0.8125\n",
      " STD: 0.04\n",
      "\n",
      "Grand average accuracy across all subjects: 0.9379\n",
      "\n",
      "Saved results to: /Users/juliette/Desktop/thesis/results/hybrid_simple/alpha+c-vep/cvep_alpha_hybrid_accuracy_results.npz\n"
     ]
    }
   ],
   "source": [
    "# -- c-VEP AND ALPHA\n",
    "\n",
    "# Paths\n",
    "cvep_features_dir = '/Users/juliette/Desktop/thesis/features/c-VEP'  # where c-VEP features are stored\n",
    "alpha_features_dir = '/Users/juliette/Desktop/thesis/features/alpha'  # where alpha features are stored\n",
    "p300_features_dir = '/Users/juliette/Desktop/thesis/preprocessing/features/with_ICA'  # where P300 features are stored\n",
    "save_dir = '/Users/juliette/Desktop/thesis/results/hybrid_simple/alpha+c-vep'\n",
    "\n",
    "\n",
    "# Subject and task lists\n",
    "subjects = [\n",
    "    \"VPpdia\", \"VPpdib\", \"VPpdic\", \"VPpdid\", \"VPpdie\", \"VPpdif\", \"VPpdig\", \"VPpdih\", \"VPpdii\", \"VPpdij\", \"VPpdik\",\n",
    "    \"VPpdil\", \"VPpdim\", \"VPpdin\", \"VPpdio\", \"VPpdip\", \"VPpdiq\", \"VPpdir\", \"VPpdis\", \"VPpdit\", \"VPpdiu\", \"VPpdiv\",\n",
    "    \"VPpdiw\", \"VPpdix\", \"VPpdiy\", \"VPpdiz\", \"VPpdiza\", \"VPpdizb\", \"VPpdizc\"\n",
    "]\n",
    "tasks = [\"covert\"]\n",
    "subject_accuracies = []\n",
    "subject_std = []\n",
    "\n",
    "\n",
    "# Loop over subjects and tasks\n",
    "for subject in subjects:\n",
    "    for task in tasks:\n",
    "        print(f\"Loading features for {subject}, task={task}\")\n",
    "\n",
    "        # Load c-VEP features\n",
    "        cvep_file_path = os.path.join(cvep_features_dir, f\"sub-{subject}_task-{task}_features.npz\")\n",
    "        if os.path.exists(cvep_file_path):\n",
    "            cvep_data = np.load(cvep_file_path)\n",
    "            X_cvep = cvep_data['features']  # Features for c-VEP task (trials x features)\n",
    "            y_cvep = cvep_data['y']  # Labels for c-VEP task\n",
    "            print(f\"Loaded c-VEP features: {X_cvep.shape}\")\n",
    "        else:\n",
    "            print(f\"Warning: c-VEP features file not found for {subject}, task={task}\")\n",
    "\n",
    "        # Load alpha features\n",
    "        alpha_file_path = os.path.join(alpha_features_dir, f\"sub-{subject}_task-{task}_alpha_features.npz\")\n",
    "        if os.path.exists(alpha_file_path):\n",
    "            alpha_data = np.load(alpha_file_path)\n",
    "            X_alpha = alpha_data['features']  # Features for alpha task (trials x features)\n",
    "            y_alpha = alpha_data['labels']  # Labels for alpha task\n",
    "            print(f\"Loaded alpha features: {X_alpha.shape}\")\n",
    "        else:\n",
    "            print(f\"Warning: alpha features file not found for {subject}, task={task}\")\n",
    "\n",
    "        # Concatenate the features of the P300, alpha and c-VEP\n",
    "        X_combined = np.concatenate([X_alpha, X_cvep], axis=1)\n",
    "\n",
    "        # Cross-validation setup\n",
    "        fold_accuracies = []\n",
    "        fold_roc_auc = []\n",
    "        n_folds = 4\n",
    "        n_trials = X_combined.shape[0] // n_folds\n",
    "        folds = np.repeat(np.arange(n_folds), n_trials)\n",
    "\n",
    "        for i_fold in range(n_folds):\n",
    "            print(f\"  Fold {i_fold + 1}/{n_folds}\")\n",
    "\n",
    "            # Split train and test data\n",
    "            X_trn, y_trn = X_combined[folds != i_fold, :], y_cvep[folds != i_fold]\n",
    "            X_tst, y_tst = X_combined[folds == i_fold, :], y_cvep[folds == i_fold]\n",
    "\n",
    "            # LDA classifier\n",
    "            lda = LinearDiscriminantAnalysis(solver=\"lsqr\", covariance_estimator=LedoitWolf())\n",
    "            lda.fit(X_trn, y_trn)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = lda.predict(X_tst)\n",
    "\n",
    "            # Compute performance metrics\n",
    "            accuracy = accuracy_score(y_tst, y_pred)\n",
    "            print(f\"Fold {i_fold+1} accuracy: {accuracy}\")\n",
    "\n",
    "            fold_accuracies.append(accuracy)\n",
    "\n",
    "        # Calculate average accuracy over all folds\n",
    "        average_accuracy = np.mean(fold_accuracies)\n",
    "        subject_accuracies.append(fold_accuracies)\n",
    "        subject_std.append(np.std(fold_accuracies))  # compute std over folds\n",
    "        print(f\"Average accuracy over all folds: {average_accuracy}\\n STD: {np.std(fold_accuracies):.2f}\")\n",
    "\n",
    "grand_average_accuracy = np.mean(subject_accuracies)\n",
    "print(f\"\\nGrand average accuracy across all subjects: {grand_average_accuracy:.4f}\")\n",
    "\n",
    "# Convert list to array\n",
    "subject_accuracies_array = np.array(subject_accuracies)\n",
    "subject_std_array = np.array(subject_std)\n",
    "\n",
    "# Save the results\n",
    "save_path = os.path.join(save_dir, 'cvep_alpha_hybrid_accuracy_results.npz')\n",
    "np.savez(save_path,\n",
    "         accuracy=subject_accuracies_array,\n",
    "         std=subject_std_array,\n",
    "         subjects=subjects,\n",
    "         tasks=tasks,\n",
    "         n_folds=n_folds,\n",
    "         method='hybrid')\n",
    "\n",
    "print(f\"\\nSaved results to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b800a648",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlattened (2*channels, features):\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mch\u001b[49m, f)\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlattened (channels, 2*features):\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mreshape(ch, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m f)\u001b[38;5;241m.\u001b[39mflatten())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ch' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Flattened (2*channels, features):\", X.reshape(2 * ch, f).flatten())\n",
    "print(\"Flattened (channels, 2*features):\", X.reshape(ch, 2 * f).flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dc9c804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features for VPpdia, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 62, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.4\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.6\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.65\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.4\n",
      "Average accuracy over all folds: 0.5125\n",
      " STD: 0.11\n",
      "Loading features for VPpdib, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 61, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 1.0\n",
      " STD: 0.00\n",
      "Loading features for VPpdic, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.85\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.9625\n",
      " STD: 0.06\n",
      "Loading features for VPpdid, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.9\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.7\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.8\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.75\n",
      "Average accuracy over all folds: 0.7875000000000001\n",
      " STD: 0.07\n",
      "Loading features for VPpdie, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.7\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.8\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.95\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.8625\n",
      " STD: 0.12\n",
      "Loading features for VPpdif, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 62, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.9\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.9625\n",
      " STD: 0.04\n",
      "Loading features for VPpdig, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.95\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.95\n",
      "Average accuracy over all folds: 0.9624999999999999\n",
      " STD: 0.02\n",
      "Loading features for VPpdih, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 62, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.95\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.9624999999999999\n",
      " STD: 0.04\n",
      "Loading features for VPpdii, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.95\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.9875\n",
      " STD: 0.02\n",
      "Loading features for VPpdij, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 62, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.75\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.85\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.8\n",
      "Average accuracy over all folds: 0.825\n",
      " STD: 0.06\n",
      "Loading features for VPpdik, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.85\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.7\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.75\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.75\n",
      "Average accuracy over all folds: 0.7625\n",
      " STD: 0.05\n",
      "Loading features for VPpdil, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.9\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.975\n",
      " STD: 0.04\n",
      "Loading features for VPpdim, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 1.0\n",
      " STD: 0.00\n",
      "Loading features for VPpdin, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 62, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.9875\n",
      " STD: 0.02\n",
      "Loading features for VPpdio, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.9\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.975\n",
      " STD: 0.04\n",
      "Loading features for VPpdip, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.9\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.9\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.95\n",
      " STD: 0.05\n",
      "Loading features for VPpdiq, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.9\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.95\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.95\n",
      "Average accuracy over all folds: 0.9375\n",
      " STD: 0.02\n",
      "Loading features for VPpdir, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.9\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.85\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.9\n",
      "Average accuracy over all folds: 0.8875\n",
      " STD: 0.02\n",
      "Loading features for VPpdis, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.975\n",
      " STD: 0.04\n",
      "Loading features for VPpdit, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 1.0\n",
      " STD: 0.00\n",
      "Loading features for VPpdiu, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.65\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.8\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.95\n",
      "Average accuracy over all folds: 0.8375000000000001\n",
      " STD: 0.12\n",
      "Loading features for VPpdiv, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 60, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.95\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.9875\n",
      " STD: 0.02\n",
      "Loading features for VPpdiw, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 59, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.95\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.95\n",
      "Average accuracy over all folds: 0.95\n",
      " STD: 0.04\n",
      "Loading features for VPpdix, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 1.0\n",
      " STD: 0.00\n",
      "Loading features for VPpdiy, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.85\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.95\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.9\n",
      "Average accuracy over all folds: 0.9125\n",
      " STD: 0.04\n",
      "Loading features for VPpdiz, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 62, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.75\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.95\n",
      "Average accuracy over all folds: 0.8999999999999999\n",
      " STD: 0.09\n",
      "Loading features for VPpdiza, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.9\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 1.0\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.975\n",
      " STD: 0.04\n",
      "Loading features for VPpdizb, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.95\n",
      "Average accuracy over all folds: 0.9624999999999999\n",
      " STD: 0.04\n",
      "Loading features for VPpdizc, task=covert\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 58, 6)\n",
      "  Fold 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 accuracy: 0.6\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.6\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.7\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.65\n",
      "Average accuracy over all folds: 0.6375\n",
      " STD: 0.04\n",
      "\n",
      "Grand average accuracy across all subjects: 0.9116\n",
      "\n",
      "Saved results to: /Users/juliette/Desktop/thesis/results/hybrid_simple/alpha+p300/p300_alpha_hybrid_accuracy_results.npz\n"
     ]
    }
   ],
   "source": [
    "# -- ALPHA AND P300\n",
    "\n",
    "# Paths\n",
    "cvep_features_dir = '/Users/juliette/Desktop/thesis/features/c-VEP'  # where c-VEP features are stored\n",
    "alpha_features_dir = '/Users/juliette/Desktop/thesis/features/alpha'  # where alpha features are stored\n",
    "p300_features_dir = '/Users/juliette/Desktop/thesis/preprocessing/features/with_ICA'  # where P300 features are stored\n",
    "save_dir = '/Users/juliette/Desktop/thesis/results/hybrid_simple/alpha+p300'\n",
    "\n",
    "\n",
    "# Subject and task lists\n",
    "subjects = [\n",
    "    \"VPpdia\", \"VPpdib\", \"VPpdic\", \"VPpdid\", \"VPpdie\", \"VPpdif\", \"VPpdig\", \"VPpdih\", \"VPpdii\", \"VPpdij\", \"VPpdik\",\n",
    "    \"VPpdil\", \"VPpdim\", \"VPpdin\", \"VPpdio\", \"VPpdip\", \"VPpdiq\", \"VPpdir\", \"VPpdis\", \"VPpdit\", \"VPpdiu\", \"VPpdiv\",\n",
    "    \"VPpdiw\", \"VPpdix\", \"VPpdiy\", \"VPpdiz\", \"VPpdiza\", \"VPpdizb\", \"VPpdizc\"\n",
    "]\n",
    "tasks = [\"covert\"]\n",
    "subject_accuracies = []\n",
    "subject_std = []\n",
    "\n",
    "# Loop over subjects and tasks\n",
    "for subject in subjects:\n",
    "    for task in tasks:\n",
    "        print(f\"Loading features for {subject}, task={task}\")\n",
    "\n",
    "        # Load alpha features\n",
    "        alpha_file_path = os.path.join(alpha_features_dir, f\"sub-{subject}_task-{task}_alpha_features.npz\")\n",
    "        if os.path.exists(alpha_file_path):\n",
    "            alpha_data = np.load(alpha_file_path)\n",
    "            X_alpha = alpha_data['features']  # Features for alpha task (trials x features)\n",
    "            y_alpha = alpha_data['labels']  # Labels for alpha task\n",
    "            print(f\"Loaded alpha features: {X_alpha.shape}\")\n",
    "        else:\n",
    "            print(f\"Warning: alpha features file not found for {subject}, task={task}\")\n",
    "\n",
    "        # Load P300 features\n",
    "        file_path_p300 = os.path.join(p300_features_dir, f\"sub-{subject}\", f\"sub-{subject}_task-{task}_p300_features_ICA.npz\")\n",
    "        if os.path.exists(file_path_p300):\n",
    "            p300_features = np.load(file_path_p300)\n",
    "\n",
    "            X_p300 = p300_features['X']  # Shape: trials x epochs x channels x features\n",
    "            y_p300 = p300_features['y']  # Labels indicating cued side: trials\n",
    "            z_p300 = p300_features['z']  # Left and right targets: trials x epochs x sides\n",
    "            fs_p300 = p300_features['fs']\n",
    "            print(f\"Loaded p300 features: {X_p300.shape}\")\n",
    "\n",
    "        # Flatten z_p300 to (trials * epochs, sides)\n",
    "        z_p300_flat = z_p300.reshape(-1, z_p300.shape[2])  # Shape: (trials * epochs, sides)\n",
    "\n",
    "        # Find the number of epochs per trial\n",
    "        epochs_per_trial = X_p300.shape[1]  # Number of epochs per trial\n",
    "\n",
    "        # Initialize lists to store averaged epochs for each trial\n",
    "        left_target_averaged_trials = []\n",
    "        right_target_averaged_trials = []\n",
    "\n",
    "        # Loop over each trial\n",
    "        for i_trial in range(X_p300.shape[0]):\n",
    "            # Extract the epochs for the current trial (shape: epochs x channels x features)\n",
    "            trial_epochs = X_p300[i_trial]\n",
    "\n",
    "            # Extract the target labels for the current trial (shape: epochs x 2)\n",
    "            trial_targets = z_p300_flat[i_trial * epochs_per_trial: (i_trial + 1) * epochs_per_trial]\n",
    "\n",
    "            # Average epochs for left target (assuming 1 = left target)\n",
    "            left_target_epochs = trial_epochs[trial_targets[:, 0] == 1]\n",
    "            if len(left_target_epochs) > 0:\n",
    "                left_target_averaged = np.mean(left_target_epochs, axis=0)  # Average across epochs (axis=0)\n",
    "            else:\n",
    "                left_target_averaged = np.zeros(trial_epochs.shape[1:])  # If no left target, set to zeros\n",
    "\n",
    "            # Average epochs for right target (assuming 1 = right target)\n",
    "            right_target_epochs = trial_epochs[trial_targets[:, 1] == 1]\n",
    "            if len(right_target_epochs) > 0:\n",
    "                right_target_averaged = np.mean(right_target_epochs, axis=0)  # Average across epochs (axis=0)\n",
    "            else:\n",
    "                right_target_averaged = np.zeros(trial_epochs.shape[1:])  # If no right target, set to zeros\n",
    "\n",
    "            # Store the averaged epochs for the current trial\n",
    "            left_target_averaged_trials.append(left_target_averaged)\n",
    "            right_target_averaged_trials.append(right_target_averaged)\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        left_target_averaged_trials = np.array(left_target_averaged_trials)\n",
    "        right_target_averaged_trials = np.array(right_target_averaged_trials)\n",
    "\n",
    "        # Concatenate the averaged features from both groups (left and right)\n",
    "        X_p300_averaged = np.concatenate([left_target_averaged_trials, right_target_averaged_trials], axis=1)\n",
    "\n",
    "        # Flatten the last two dimensions (channels and features)\n",
    "        X_p300_averaged_flat = X_p300_averaged.reshape(X_p300_averaged.shape[0], -1)\n",
    "        \n",
    "        # Concatenate the features of the P300, alpha and c-VEP\n",
    "        X_combined = np.concatenate([X_p300_averaged_flat, X_alpha], axis=1)\n",
    "\n",
    "        # Cross-validation setup\n",
    "        fold_accuracies = []\n",
    "        fold_roc_auc = []\n",
    "        n_folds = 4\n",
    "        n_trials = X_combined.shape[0] // n_folds\n",
    "        folds = np.repeat(np.arange(n_folds), n_trials)\n",
    "\n",
    "        for i_fold in range(n_folds):\n",
    "            print(f\"  Fold {i_fold + 1}/{n_folds}\")\n",
    "\n",
    "            # Split train and test data\n",
    "            X_trn, y_trn = X_combined[folds != i_fold, :], y_p300[folds != i_fold]\n",
    "            X_tst, y_tst = X_combined[folds == i_fold, :], y_p300[folds == i_fold]\n",
    "\n",
    "            # LDA classifier\n",
    "            lda = LinearDiscriminantAnalysis(solver=\"lsqr\", covariance_estimator=LedoitWolf())\n",
    "            lda.fit(X_trn, y_trn)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = lda.predict(X_tst)\n",
    "\n",
    "            # Compute performance metrics\n",
    "            accuracy = accuracy_score(y_tst, y_pred)\n",
    "            print(f\"Fold {i_fold+1} accuracy: {accuracy}\")\n",
    "\n",
    "            fold_accuracies.append(accuracy)\n",
    "\n",
    "        # Calculate average accuracy over all folds\n",
    "        average_accuracy = np.mean(fold_accuracies)\n",
    "        subject_accuracies.append(fold_accuracies)\n",
    "        subject_std.append(np.std(fold_accuracies))  # compute std over folds\n",
    "        print(f\"Average accuracy over all folds: {average_accuracy}\\n STD: {np.std(fold_accuracies):.2f}\")\n",
    "\n",
    "grand_average_accuracy = np.mean(subject_accuracies)\n",
    "print(f\"\\nGrand average accuracy across all subjects: {grand_average_accuracy:.4f}\")\n",
    "\n",
    "# Convert list to array\n",
    "subject_accuracies_array = np.array(subject_accuracies)\n",
    "subject_std_array = np.array(subject_std)\n",
    "\n",
    "# Save the results\n",
    "save_path = os.path.join(save_dir, 'p300_alpha_hybrid_accuracy_results.npz')\n",
    "np.savez(save_path,\n",
    "         accuracy=subject_accuracies_array,\n",
    "         std=subject_std_array,\n",
    "         subjects=subjects,\n",
    "         tasks=tasks,\n",
    "         n_folds=n_folds,\n",
    "         method='hybrid')\n",
    "\n",
    "print(f\"\\nSaved results to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f584edd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features for VPpdia, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 62, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.65\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.7\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.7\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.8\n",
      "Average accuracy over all folds: 0.7124999999999999\n",
      " STD: 0.05\n",
      "Loading features for VPpdib, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 61, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.75\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.65\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.85\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.65\n",
      "Average accuracy over all folds: 0.725\n",
      " STD: 0.08\n",
      "Loading features for VPpdic, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.55\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.65\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.6\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.75\n",
      "Average accuracy over all folds: 0.6375000000000001\n",
      " STD: 0.07\n",
      "Loading features for VPpdid, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.8\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.9\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.85\n",
      "Average accuracy over all folds: 0.8625\n",
      " STD: 0.04\n",
      "Loading features for VPpdie, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.75\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.95\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.7\n",
      "Average accuracy over all folds: 0.825\n",
      " STD: 0.10\n",
      "Loading features for VPpdif, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 62, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.65\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.8\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.85\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.8\n",
      "Average accuracy over all folds: 0.7750000000000001\n",
      " STD: 0.07\n",
      "Loading features for VPpdig, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.9\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.9\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.8\n",
      "Average accuracy over all folds: 0.8875\n",
      " STD: 0.05\n",
      "Loading features for VPpdih, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 62, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.85\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.8\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.8\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.95\n",
      "Average accuracy over all folds: 0.8500000000000001\n",
      " STD: 0.06\n",
      "Loading features for VPpdii, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.85\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.75\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.85\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.75\n",
      "Average accuracy over all folds: 0.8\n",
      " STD: 0.05\n",
      "Loading features for VPpdij, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 62, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.95\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.8\n",
      "Average accuracy over all folds: 0.9124999999999999\n",
      " STD: 0.07\n",
      "Loading features for VPpdik, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.85\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.65\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.9\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.9\n",
      "Average accuracy over all folds: 0.825\n",
      " STD: 0.10\n",
      "Loading features for VPpdil, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.8\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.75\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.7\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.9\n",
      "Average accuracy over all folds: 0.7875\n",
      " STD: 0.07\n",
      "Loading features for VPpdim, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.75\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.8\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.85\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.75\n",
      "Average accuracy over all folds: 0.7875\n",
      " STD: 0.04\n",
      "Loading features for VPpdin, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 62, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.8\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.9\n",
      "Average accuracy over all folds: 0.9\n",
      " STD: 0.07\n",
      "Loading features for VPpdio, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.7\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 1.0\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.9\n",
      "Average accuracy over all folds: 0.8875\n",
      " STD: 0.11\n",
      "Loading features for VPpdip, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 1.0\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.85\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.8\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.9\n",
      "Average accuracy over all folds: 0.8875000000000001\n",
      " STD: 0.07\n",
      "Loading features for VPpdiq, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.75\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.8\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.65\n",
      "Average accuracy over all folds: 0.775\n",
      " STD: 0.09\n",
      "Loading features for VPpdir, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.85\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.9\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.8\n",
      "Average accuracy over all folds: 0.8625\n",
      " STD: 0.04\n",
      "Loading features for VPpdis, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.95\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.7\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.8\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.75\n",
      "Average accuracy over all folds: 0.8\n",
      " STD: 0.09\n",
      "Loading features for VPpdit, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.75\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.85\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.75\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.9\n",
      "Average accuracy over all folds: 0.8125\n",
      " STD: 0.06\n",
      "Loading features for VPpdiu, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.9\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.75\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.75\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.95\n",
      "Average accuracy over all folds: 0.8374999999999999\n",
      " STD: 0.09\n",
      "Loading features for VPpdiv, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 60, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.85\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.85\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.85\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 1.0\n",
      "Average accuracy over all folds: 0.8875\n",
      " STD: 0.06\n",
      "Loading features for VPpdiw, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 59, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.9\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.75\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.85\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.7\n",
      "Average accuracy over all folds: 0.8\n",
      " STD: 0.08\n",
      "Loading features for VPpdix, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.75\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.9\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.75\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.95\n",
      "Average accuracy over all folds: 0.8374999999999999\n",
      " STD: 0.09\n",
      "Loading features for VPpdiy, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 64, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.75\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.85\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.8\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.9\n",
      "Average accuracy over all folds: 0.8250000000000001\n",
      " STD: 0.06\n",
      "Loading features for VPpdiz, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 62, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.75\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.85\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.9\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.5\n",
      "Average accuracy over all folds: 0.75\n",
      " STD: 0.15\n",
      "Loading features for VPpdiza, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.75\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.75\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.65\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.8\n",
      "Average accuracy over all folds: 0.7375\n",
      " STD: 0.05\n",
      "Loading features for VPpdizb, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 63, 6)\n",
      "  Fold 1/4\n",
      "Fold 1 accuracy: 0.75\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.75\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.85\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.7\n",
      "Average accuracy over all folds: 0.7625\n",
      " STD: 0.05\n",
      "Loading features for VPpdizc, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded p300 features: (80, 80, 58, 6)\n",
      "  Fold 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 accuracy: 0.95\n",
      "  Fold 2/4\n",
      "Fold 2 accuracy: 0.95\n",
      "  Fold 3/4\n",
      "Fold 3 accuracy: 0.95\n",
      "  Fold 4/4\n",
      "Fold 4 accuracy: 0.85\n",
      "Average accuracy over all folds: 0.9249999999999999\n",
      " STD: 0.04\n",
      "\n",
      "Grand average accuracy across all subjects: 0.8164\n",
      "\n",
      "Saved results to: /Users/juliette/Desktop/thesis/results/hybrid_simple/p300+cvep/p300_cvep_hybrid_accuracy_results.npz\n"
     ]
    }
   ],
   "source": [
    "# -- c-VEP AND P300\n",
    "\n",
    "# Paths\n",
    "cvep_features_dir = '/Users/juliette/Desktop/thesis/features/c-VEP'  # where c-VEP features are stored\n",
    "alpha_features_dir = '/Users/juliette/Desktop/thesis/features/alpha'  # where alpha features are stored\n",
    "p300_features_dir = '/Users/juliette/Desktop/thesis/preprocessing/features/with_ICA'  # where P300 features are stored\n",
    "save_dir = '/Users/juliette/Desktop/thesis/results/hybrid_simple/p300+cvep'\n",
    "subject_std = []\n",
    "\n",
    "# Subject and task lists\n",
    "subjects = [\n",
    "    \"VPpdia\", \"VPpdib\", \"VPpdic\", \"VPpdid\", \"VPpdie\", \"VPpdif\", \"VPpdig\", \"VPpdih\", \"VPpdii\", \"VPpdij\", \"VPpdik\",\n",
    "    \"VPpdil\", \"VPpdim\", \"VPpdin\", \"VPpdio\", \"VPpdip\", \"VPpdiq\", \"VPpdir\", \"VPpdis\", \"VPpdit\", \"VPpdiu\", \"VPpdiv\",\n",
    "    \"VPpdiw\", \"VPpdix\", \"VPpdiy\", \"VPpdiz\", \"VPpdiza\", \"VPpdizb\", \"VPpdizc\"\n",
    "]\n",
    "tasks = [\"covert\"]\n",
    "subject_accuracies = []\n",
    "\n",
    "# Loop over subjects and tasks\n",
    "for subject in subjects:\n",
    "    for task in tasks:\n",
    "        print(f\"Loading features for {subject}, task={task}\")\n",
    "\n",
    "        # Load c-VEP features\n",
    "        cvep_file_path = os.path.join(cvep_features_dir, f\"sub-{subject}_task-{task}_features.npz\")\n",
    "        if os.path.exists(cvep_file_path):\n",
    "            cvep_data = np.load(cvep_file_path)\n",
    "            X_cvep = cvep_data['features']  # Features for c-VEP task (trials x features)\n",
    "            y_cvep = cvep_data['y']  # Labels for c-VEP task\n",
    "            print(f\"Loaded c-VEP features: {X_cvep.shape}\")\n",
    "        else:\n",
    "            print(f\"Warning: c-VEP features file not found for {subject}, task={task}\")\n",
    "\n",
    "        # Load P300 features\n",
    "        file_path_p300 = os.path.join(p300_features_dir, f\"sub-{subject}\", f\"sub-{subject}_task-{task}_p300_features_ICA.npz\")\n",
    "        if os.path.exists(file_path_p300):\n",
    "            p300_features = np.load(file_path_p300)\n",
    "\n",
    "            X_p300 = p300_features['X']  # Shape: trials x epochs x channels x features\n",
    "            y_p300 = p300_features['y']  # Labels indicating cued side: trials\n",
    "            z_p300 = p300_features['z']  # Left and right targets: trials x epochs x sides\n",
    "            fs_p300 = p300_features['fs']\n",
    "            print(f\"Loaded p300 features: {X_p300.shape}\")\n",
    "\n",
    "        # Flatten z_p300 to (trials * epochs, sides)\n",
    "        z_p300_flat = z_p300.reshape(-1, z_p300.shape[2])  # Shape: (trials * epochs, sides)\n",
    "\n",
    "        # Find the number of epochs per trial\n",
    "        epochs_per_trial = X_p300.shape[1]  # Number of epochs per trial\n",
    "\n",
    "        # Initialize lists to store averaged epochs for each trial\n",
    "        left_target_averaged_trials = []\n",
    "        right_target_averaged_trials = []\n",
    "\n",
    "        # Loop over each trial\n",
    "        for i_trial in range(X_p300.shape[0]):\n",
    "            # Extract the epochs for the current trial (shape: epochs x channels x features)\n",
    "            trial_epochs = X_p300[i_trial]\n",
    "\n",
    "            # Extract the target labels for the current trial (shape: epochs x 2)\n",
    "            trial_targets = z_p300_flat[i_trial * epochs_per_trial: (i_trial + 1) * epochs_per_trial]\n",
    "\n",
    "            # Average epochs for left target (assuming 1 = left target)\n",
    "            left_target_epochs = trial_epochs[trial_targets[:, 0] == 1]\n",
    "            if len(left_target_epochs) > 0:\n",
    "                left_target_averaged = np.mean(left_target_epochs, axis=0)  # Average across epochs (axis=0)\n",
    "            else:\n",
    "                left_target_averaged = np.zeros(trial_epochs.shape[1:])  # If no left target, set to zeros\n",
    "\n",
    "            # Average epochs for right target (assuming 1 = right target)\n",
    "            right_target_epochs = trial_epochs[trial_targets[:, 1] == 1]\n",
    "            if len(right_target_epochs) > 0:\n",
    "                right_target_averaged = np.mean(right_target_epochs, axis=0)  # Average across epochs (axis=0)\n",
    "            else:\n",
    "                right_target_averaged = np.zeros(trial_epochs.shape[1:])  # If no right target, set to zeros\n",
    "\n",
    "            # Store the averaged epochs for the current trial\n",
    "            left_target_averaged_trials.append(left_target_averaged)\n",
    "            right_target_averaged_trials.append(right_target_averaged)\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        left_target_averaged_trials = np.array(left_target_averaged_trials)\n",
    "        right_target_averaged_trials = np.array(right_target_averaged_trials)\n",
    "\n",
    "        # Concatenate the averaged features from both groups (left and right)\n",
    "        X_p300_averaged = np.concatenate([left_target_averaged_trials, right_target_averaged_trials], axis=1)\n",
    "\n",
    "        # Flatten the last two dimensions (channels and features)\n",
    "        X_p300_averaged_flat = X_p300_averaged.reshape(X_p300_averaged.shape[0], -1)\n",
    "        \n",
    "        # Concatenate the features of the P300, alpha and c-VEP\n",
    "        X_combined = np.concatenate([X_p300_averaged_flat, X_alpha, X_cvep], axis=1)\n",
    "\n",
    "        # Cross-validation setup\n",
    "        fold_accuracies = []\n",
    "        fold_roc_auc = []\n",
    "        n_folds = 4\n",
    "        n_trials = X_combined.shape[0] // n_folds\n",
    "        folds = np.repeat(np.arange(n_folds), n_trials)\n",
    "\n",
    "        for i_fold in range(n_folds):\n",
    "            print(f\"  Fold {i_fold + 1}/{n_folds}\")\n",
    "\n",
    "            # Split train and test data\n",
    "            X_trn, y_trn = X_combined[folds != i_fold, :], y_cvep[folds != i_fold]\n",
    "            X_tst, y_tst = X_combined[folds == i_fold, :], y_cvep[folds == i_fold]\n",
    "\n",
    "            # LDA classifier\n",
    "            lda = LinearDiscriminantAnalysis(solver=\"lsqr\", covariance_estimator=LedoitWolf())\n",
    "            lda.fit(X_trn, y_trn)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = lda.predict(X_tst)\n",
    "\n",
    "            # Compute performance metrics\n",
    "            accuracy = accuracy_score(y_tst, y_pred)\n",
    "            print(f\"Fold {i_fold+1} accuracy: {accuracy}\")\n",
    "\n",
    "            fold_accuracies.append(accuracy)\n",
    "\n",
    "        # Calculate average accuracy over all folds\n",
    "        average_accuracy = np.mean(fold_accuracies)\n",
    "        subject_accuracies.append(fold_accuracies)\n",
    "        subject_std.append(np.std(fold_accuracies))  # compute std over folds\n",
    "        print(f\"Average accuracy over all folds: {average_accuracy}\\n STD: {np.std(fold_accuracies):.2f}\")\n",
    "\n",
    "grand_average_accuracy = np.mean(subject_accuracies)\n",
    "print(f\"\\nGrand average accuracy across all subjects: {grand_average_accuracy:.4f}\")\n",
    "\n",
    "# Convert list to array\n",
    "subject_accuracies_array = np.array(subject_accuracies)\n",
    "subject_std_array = np.array(subject_std)\n",
    "\n",
    "# Save the results\n",
    "save_path = os.path.join(save_dir, 'p300_cvep_hybrid_accuracy_results.npz')\n",
    "np.savez(save_path,\n",
    "         accuracy=subject_accuracies_array,\n",
    "         std=subject_std_array,\n",
    "         subjects=subjects,\n",
    "         tasks=tasks,\n",
    "         n_folds=n_folds,\n",
    "         method='hybrid')\n",
    "\n",
    "print(f\"\\nSaved results to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc72497",
   "metadata": {},
   "source": [
    "## Using BT-LDA\n",
    "Starting with hybrid decoding, now using BT-LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f74b3059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features for VPpdia, task=covert\n",
      "Loaded c-VEP features: (80, 2)\n",
      "Loaded alpha features: (80, 4)\n",
      "Loaded p300 features: (80, 80, 62, 6)\n",
      "SHAPE of X_p300_averaged_flat: (80, 744)\n",
      "  Fold 1/4\n",
      "SHAPE OF X_tst: (20, 750)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 123\u001b[0m\n\u001b[1;32m    121\u001b[0m toeplitz \u001b[38;5;241m=\u001b[39m ToeplitzLDA(n_channels\u001b[38;5;241m=\u001b[39mn_channels)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Dimensionality bust be 2\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[43mtoeplitz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m    126\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m lda\u001b[38;5;241m.\u001b[39mpredict(X_tst)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/toeplitzlda/classification/toeplitzlda.py:180\u001b[0m, in \u001b[0;36mShrinkageLinearDiscriminantAnalysis.fit\u001b[0;34m(self, X_train, y, oracle_data)\u001b[0m\n\u001b[1;32m    178\u001b[0m         C_cov \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m priors[cur_class] \u001b[38;5;241m*\u001b[39m shrinkage(x_slice)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    179\u001b[0m dim \u001b[38;5;241m=\u001b[39m C_cov\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 180\u001b[0m nt \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_n_times\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_times\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m stm \u001b[38;5;241m=\u001b[39m SpatioTemporalMatrix(\n\u001b[1;32m    182\u001b[0m     C_cov,\n\u001b[1;32m    183\u001b[0m     n_chans\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_channels,\n\u001b[1;32m    184\u001b[0m     n_times\u001b[38;5;241m=\u001b[39mnt,\n\u001b[1;32m    185\u001b[0m     channel_prime\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_is_channel_prime,\n\u001b[1;32m    186\u001b[0m )\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_is_channel_prime:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/toeplitzlda/classification/covariance.py:83\u001b[0m, in \u001b[0;36mcalc_n_times\u001b[0;34m(dim, n_channels, n_times)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m n_times\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m n_times \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m%\u001b[39m n_channels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not infer time samples. Remainder is non-zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "from toeplitzlda.classification import ToeplitzLDA\n",
    "\n",
    "# Paths\n",
    "cvep_features_dir = '/Users/juliette/Desktop/thesis/features/c-VEP'  # where c-VEP features are stored\n",
    "alpha_features_dir = '/Users/juliette/Desktop/thesis/features/alpha'  # where alpha features are stored\n",
    "p300_features_dir = '/Users/juliette/Desktop/thesis/preprocessing/features/with_ICA'  # where P300 features are stored\n",
    "save_dir = '/Users/juliette/Desktop/thesis/results/hybrid_simple/alpha+p300+c-vep'\n",
    "\n",
    "# Subject and task lists\n",
    "subjects = [\n",
    "    \"VPpdia\", \"VPpdib\", \"VPpdic\", \"VPpdid\", \"VPpdie\", \"VPpdif\", \"VPpdig\", \"VPpdih\", \"VPpdii\", \"VPpdij\", \"VPpdik\",\n",
    "    \"VPpdil\", \"VPpdim\", \"VPpdin\", \"VPpdio\", \"VPpdip\", \"VPpdiq\", \"VPpdir\", \"VPpdis\", \"VPpdit\", \"VPpdiu\", \"VPpdiv\",\n",
    "    \"VPpdiw\", \"VPpdix\", \"VPpdiy\", \"VPpdiz\", \"VPpdiza\", \"VPpdizb\", \"VPpdizc\"\n",
    "]\n",
    "tasks = [\"covert\"]\n",
    "subject_accuracies = []\n",
    "subject_std = []\n",
    "\n",
    "# Loop over subjects and tasks\n",
    "for subject in subjects:\n",
    "    for task in tasks:\n",
    "        print(f\"Loading features for {subject}, task={task}\")\n",
    "\n",
    "        # Load c-VEP features\n",
    "        cvep_file_path = os.path.join(cvep_features_dir, f\"sub-{subject}_task-{task}_features.npz\")\n",
    "        if os.path.exists(cvep_file_path):\n",
    "            cvep_data = np.load(cvep_file_path)\n",
    "            X_cvep = cvep_data['features']  # Features for c-VEP task (trials x features)\n",
    "            y_cvep = cvep_data['y']  # Labels for c-VEP task\n",
    "            print(f\"Loaded c-VEP features: {X_cvep.shape}\")\n",
    "        else:\n",
    "            print(f\"Warning: c-VEP features file not found for {subject}, task={task}\")\n",
    "\n",
    "        # Load alpha features\n",
    "        alpha_file_path = os.path.join(alpha_features_dir, f\"sub-{subject}_task-{task}_alpha_features.npz\")\n",
    "        if os.path.exists(alpha_file_path):\n",
    "            alpha_data = np.load(alpha_file_path)\n",
    "            X_alpha = alpha_data['features']  # Features for alpha task (trials x features)\n",
    "            y_alpha = alpha_data['labels']  # Labels for alpha task\n",
    "            print(f\"Loaded alpha features: {X_alpha.shape}\")\n",
    "        else:\n",
    "            print(f\"Warning: alpha features file not found for {subject}, task={task}\")\n",
    "\n",
    "        # Load P300 features\n",
    "        file_path_p300 = os.path.join(p300_features_dir, f\"sub-{subject}\", f\"sub-{subject}_task-{task}_p300_features_ICA.npz\")\n",
    "        if os.path.exists(file_path_p300):\n",
    "            p300_features = np.load(file_path_p300)\n",
    "\n",
    "            X_p300 = p300_features['X']  # Shape: trials x epochs x channels x features\n",
    "            y_p300 = p300_features['y']  # Labels indicating cued side: trials\n",
    "            z_p300 = p300_features['z']  # Left and right targets: trials x epochs x sides\n",
    "            fs_p300 = p300_features['fs']\n",
    "            print(f\"Loaded p300 features: {X_p300.shape}\")\n",
    "        n_channels = X_p300[2]\n",
    "        # Flatten z_p300 to (trials * epochs, sides)\n",
    "        z_p300_flat = z_p300.reshape(-1, z_p300.shape[2])  # Shape: (trials * epochs, sides)\n",
    "\n",
    "        # Find the number of epochs per trial\n",
    "        epochs_per_trial = X_p300.shape[1]  # Number of epochs per trial\n",
    "\n",
    "        # Initialize lists to store averaged epochs for each trial\n",
    "        left_target_averaged_trials = []\n",
    "        right_target_averaged_trials = []\n",
    "\n",
    "        # Loop over each trial\n",
    "        for i_trial in range(X_p300.shape[0]):\n",
    "            # Extract the epochs for the current trial (shape: epochs x channels x features)\n",
    "            trial_epochs = X_p300[i_trial]\n",
    "\n",
    "            # Extract the target labels for the current trial (shape: epochs x 2)\n",
    "            trial_targets = z_p300_flat[i_trial * epochs_per_trial: (i_trial + 1) * epochs_per_trial]\n",
    "\n",
    "            # Average epochs for left target (assuming 1 = left target)\n",
    "            left_target_epochs = trial_epochs[trial_targets[:, 0] == 1]\n",
    "            if len(left_target_epochs) > 0:\n",
    "                left_target_averaged = np.mean(left_target_epochs, axis=0)  # Average across epochs (axis=0)\n",
    "            else:\n",
    "                left_target_averaged = np.zeros(trial_epochs.shape[1:])  # If no left target, set to zeros\n",
    "\n",
    "            # Average epochs for right target (assuming 1 = right target)\n",
    "            right_target_epochs = trial_epochs[trial_targets[:, 1] == 1]\n",
    "            if len(right_target_epochs) > 0:\n",
    "                right_target_averaged = np.mean(right_target_epochs, axis=0)  # Average across epochs (axis=0)\n",
    "            else:\n",
    "                right_target_averaged = np.zeros(trial_epochs.shape[1:])  # If no right target, set to zeros\n",
    "\n",
    "            # Store the averaged epochs for the current trial\n",
    "            left_target_averaged_trials.append(left_target_averaged)\n",
    "            right_target_averaged_trials.append(right_target_averaged)\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        left_target_averaged_trials = np.array(left_target_averaged_trials)\n",
    "        right_target_averaged_trials = np.array(right_target_averaged_trials)\n",
    "\n",
    "        # Concatenate the averaged features from both groups (left and right)\n",
    "        X_p300_averaged = np.concatenate([left_target_averaged_trials, right_target_averaged_trials], axis=1)\n",
    "\n",
    "        # Flatten the last two dimensions (channels and features)\n",
    "        X_p300_averaged_flat = X_p300_averaged.reshape(X_p300_averaged.shape[0], -1)\n",
    "        print(\"SHAPE of X_p300_averaged_flat:\", X_p300_averaged_flat.shape)\n",
    "        # Concatenate the features of the P300, alpha and c-VEP\n",
    "        X_combined = np.concatenate([X_p300_averaged_flat, X_alpha, X_cvep], axis=1)\n",
    "\n",
    "        # Cross-validation setup\n",
    "        fold_accuracies = []\n",
    "        fold_roc_auc = []\n",
    "        n_folds = 4\n",
    "        n_trials = X_combined.shape[0] // n_folds\n",
    "        folds = np.repeat(np.arange(n_folds), n_trials)\n",
    "\n",
    "        for i_fold in range(n_folds):\n",
    "            print(f\"  Fold {i_fold + 1}/{n_folds}\")\n",
    "\n",
    "            # Split train and test data\n",
    "            X_trn, y_trn = X_combined[folds != i_fold, :], y_cvep[folds != i_fold]\n",
    "            X_tst, y_tst = X_combined[folds == i_fold, :], y_cvep[folds == i_fold]\n",
    "            \n",
    "            # LDA classifier\n",
    "            ## Fit LDA\n",
    "            print(\"SHAPE OF X_tst:\", X_tst.shape)\n",
    "            toeplitz = ToeplitzLDA(n_channels=n_channels)\n",
    "            # Dimensionality bust be 2\n",
    "            toeplitz.fit(X_trn, y_trn)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = lda.predict(X_tst)\n",
    "\n",
    "            # Compute performance metrics\n",
    "            accuracy = accuracy_score(y_tst, y_pred)\n",
    "            print(f\"Fold {i_fold+1} accuracy: {accuracy}\")\n",
    "\n",
    "            fold_accuracies.append(accuracy)\n",
    "\n",
    "        # Calculate average accuracy over all folds\n",
    "        average_accuracy = np.mean(fold_accuracies)\n",
    "        subject_accuracies.append(fold_accuracies)\n",
    "        subject_std.append(np.std(fold_accuracies))  # compute std over folds\n",
    "        print(f\"Average accuracy over all folds: {average_accuracy}\\n STD: {np.std(fold_accuracies):.2f}\")\n",
    "\n",
    "grand_average_accuracy = np.mean(subject_accuracies)\n",
    "print(f\"\\nGrand average accuracy across all subjects: {grand_average_accuracy:.4f}\")\n",
    "\n",
    "# Convert list to array\n",
    "subject_accuracies_array = np.array(subject_accuracies)\n",
    "subject_std_array = np.array(subject_std)\n",
    "\n",
    "# # Save the results\n",
    "# save_path = os.path.join(save_dir, 'cvep_p300_alpha_hybrid_accuracy_results.npz')\n",
    "# np.savez(save_path,\n",
    "#          accuracy=subject_accuracies_array,\n",
    "#          std=subject_std_array,\n",
    "#          subjects=subjects,\n",
    "#          tasks=tasks,\n",
    "#          n_folds=n_folds,\n",
    "#          method='hybrid')\n",
    "\n",
    "print(f\"\\nSaved results to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469dc15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
